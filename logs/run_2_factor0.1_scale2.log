Optimizing /home/cw4287/gaussian-model/train1
Output folder: /home/cw4287/gaussian-model/train1 [14/11 19:07:44]
Tensorboard not available: not logging progress [14/11 19:07:44]
------------LLFF HOLD------------- [14/11 19:07:46]

Reading camera 1/301
Reading camera 2/301
Reading camera 3/301
Reading camera 4/301
Reading camera 5/301
Reading camera 6/301
Reading camera 7/301
Reading camera 8/301
Reading camera 9/301
Reading camera 10/301
Reading camera 11/301
Reading camera 12/301
Reading camera 13/301
Reading camera 14/301
Reading camera 15/301
Reading camera 16/301
Reading camera 17/301
Reading camera 18/301
Reading camera 19/301
Reading camera 20/301
Reading camera 21/301
Reading camera 22/301
Reading camera 23/301
Reading camera 24/301
Reading camera 25/301
Reading camera 26/301
Reading camera 27/301
Reading camera 28/301
Reading camera 29/301
Reading camera 30/301
Reading camera 31/301
Reading camera 32/301
Reading camera 33/301
Reading camera 34/301
Reading camera 35/301
Reading camera 36/301
Reading camera 37/301
Reading camera 38/301
Reading camera 39/301
Reading camera 40/301
Reading camera 41/301
Reading camera 42/301
Reading camera 43/301
Reading camera 44/301
Reading camera 45/301
Reading camera 46/301
Reading camera 47/301
Reading camera 48/301
Reading camera 49/301
Reading camera 50/301
Reading camera 51/301
Reading camera 52/301
Reading camera 53/301
Reading camera 54/301
Reading camera 55/301
Reading camera 56/301
Reading camera 57/301
Reading camera 58/301
Reading camera 59/301
Reading camera 60/301
Reading camera 61/301
Reading camera 62/301
Reading camera 63/301
Reading camera 64/301
Reading camera 65/301
Reading camera 66/301
Reading camera 67/301
Reading camera 68/301
Reading camera 69/301
Reading camera 70/301
Reading camera 71/301
Reading camera 72/301
Reading camera 73/301
Reading camera 74/301
Reading camera 75/301
Reading camera 76/301
Reading camera 77/301
Reading camera 78/301
Reading camera 79/301
Reading camera 80/301
Reading camera 81/301
Reading camera 82/301
Reading camera 83/301
Reading camera 84/301
Reading camera 85/301
Reading camera 86/301
Reading camera 87/301
Reading camera 88/301
Reading camera 89/301
Reading camera 90/301
Reading camera 91/301
Reading camera 92/301
Reading camera 93/301
Reading camera 94/301
Reading camera 95/301
Reading camera 96/301
Reading camera 97/301
Reading camera 98/301
Reading camera 99/301
Reading camera 100/301
Reading camera 101/301
Reading camera 102/301
Reading camera 103/301
Reading camera 104/301
Reading camera 105/301
Reading camera 106/301
Reading camera 107/301
Reading camera 108/301
Reading camera 109/301
Reading camera 110/301
Reading camera 111/301
Reading camera 112/301
Reading camera 113/301
Reading camera 114/301
Reading camera 115/301
Reading camera 116/301
Reading camera 117/301
Reading camera 118/301
Reading camera 119/301
Reading camera 120/301
Reading camera 121/301
Reading camera 122/301
Reading camera 123/301
Reading camera 124/301
Reading camera 125/301
Reading camera 126/301
Reading camera 127/301
Reading camera 128/301
Reading camera 129/301
Reading camera 130/301
Reading camera 131/301
Reading camera 132/301
Reading camera 133/301
Reading camera 134/301
Reading camera 135/301
Reading camera 136/301
Reading camera 137/301
Reading camera 138/301
Reading camera 139/301
Reading camera 140/301
Reading camera 141/301
Reading camera 142/301
Reading camera 143/301
Reading camera 144/301
Reading camera 145/301
Reading camera 146/301
Reading camera 147/301
Reading camera 148/301
Reading camera 149/301
Reading camera 150/301
Reading camera 151/301
Reading camera 152/301
Reading camera 153/301
Reading camera 154/301
Reading camera 155/301
Reading camera 156/301
Reading camera 157/301
Reading camera 158/301
Reading camera 159/301
Reading camera 160/301
Reading camera 161/301
Reading camera 162/301
Reading camera 163/301
Reading camera 164/301
Reading camera 165/301
Reading camera 166/301
Reading camera 167/301
Reading camera 168/301
Reading camera 169/301
Reading camera 170/301
Reading camera 171/301
Reading camera 172/301
Reading camera 173/301
Reading camera 174/301
Reading camera 175/301
Reading camera 176/301
Reading camera 177/301
Reading camera 178/301
Reading camera 179/301
Reading camera 180/301
Reading camera 181/301
Reading camera 182/301
Reading camera 183/301
Reading camera 184/301
Reading camera 185/301
Reading camera 186/301
Reading camera 187/301
Reading camera 188/301
Reading camera 189/301
Reading camera 190/301
Reading camera 191/301
Reading camera 192/301
Reading camera 193/301
Reading camera 194/301
Reading camera 195/301
Reading camera 196/301
Reading camera 197/301
Reading camera 198/301
Reading camera 199/301
Reading camera 200/301
Reading camera 201/301
Reading camera 202/301
Reading camera 203/301
Reading camera 204/301
Reading camera 205/301
Reading camera 206/301
Reading camera 207/301
Reading camera 208/301
Reading camera 209/301
Reading camera 210/301
Reading camera 211/301
Reading camera 212/301
Reading camera 213/301
Reading camera 214/301
Reading camera 215/301
Reading camera 216/301
Reading camera 217/301
Reading camera 218/301
Reading camera 219/301
Reading camera 220/301
Reading camera 221/301
Reading camera 222/301
Reading camera 223/301
Reading camera 224/301
Reading camera 225/301
Reading camera 226/301
Reading camera 227/301
Reading camera 228/301
Reading camera 229/301
Reading camera 230/301
Reading camera 231/301
Reading camera 232/301
Reading camera 233/301
Reading camera 234/301
Reading camera 235/301
Reading camera 236/301
Reading camera 237/301
Reading camera 238/301
Reading camera 239/301
Reading camera 240/301
Reading camera 241/301
Reading camera 242/301
Reading camera 243/301
Reading camera 244/301
Reading camera 245/301
Reading camera 246/301
Reading camera 247/301
Reading camera 248/301
Reading camera 249/301
Reading camera 250/301
Reading camera 251/301
Reading camera 252/301
Reading camera 253/301
Reading camera 254/301
Reading camera 255/301
Reading camera 256/301
Reading camera 257/301
Reading camera 258/301
Reading camera 259/301
Reading camera 260/301
Reading camera 261/301
Reading camera 262/301
Reading camera 263/301
Reading camera 264/301
Reading camera 265/301
Reading camera 266/301
Reading camera 267/301
Reading camera 268/301
Reading camera 269/301
Reading camera 270/301
Reading camera 271/301
Reading camera 272/301
Reading camera 273/301
Reading camera 274/301
Reading camera 275/301
Reading camera 276/301
Reading camera 277/301
Reading camera 278/301
Reading camera 279/301
Reading camera 280/301
Reading camera 281/301
Reading camera 282/301
Reading camera 283/301
Reading camera 284/301
Reading camera 285/301
Reading camera 286/301
Reading camera 287/301
Reading camera 288/301
Reading camera 289/301
Reading camera 290/301
Reading camera 291/301
Reading camera 292/301
Reading camera 293/301
Reading camera 294/301
Reading camera 295/301
Reading camera 296/301
Reading camera 297/301
Reading camera 298/301
Reading camera 299/301
Reading camera 300/301
Reading camera 301/301 [14/11 19:07:46]
Loading Training Cameras [14/11 19:07:46]
Loading Test Cameras [14/11 19:08:16]
Number of points at initialisation :  182686 [14/11 19:08:20]

Training progress:   0%|          | 0/2000 [00:00<?, ?it/s]/ext3/conda/envs/3dgs/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/ext3/conda/envs/3dgs/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
/ext3/conda/envs/3dgs/lib/python3.12/site-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at /opt/conda/conda-bld/pytorch_1712609048481/work/aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return F.conv2d(input, weight, bias, self.stride,

Training progress:   0%|          | 0/2000 [00:07<?, ?it/s, Loss=1.5182825, Gaussian number=182686, print grad=0.0006171023705974221, Depth Loss=0.0000000]
Training progress:   0%|          | 10/2000 [00:07<24:36,  1.35it/s, Loss=1.5182825, Gaussian number=182686, print grad=0.0006171023705974221, Depth Loss=0.0000000]
Training progress:   0%|          | 10/2000 [00:09<24:36,  1.35it/s, Loss=1.4706317, Gaussian number=182686, print grad=0.0016377671854570508, Depth Loss=0.0000000]
Training progress:   1%|          | 20/2000 [00:09<13:23,  2.47it/s, Loss=1.4706317, Gaussian number=182686, print grad=0.0016377671854570508, Depth Loss=0.0000000]
Training progress:   1%|          | 20/2000 [00:10<13:23,  2.47it/s, Loss=1.4598874, Gaussian number=182686, print grad=0.002598752733319998, Depth Loss=0.0000000] 
Training progress:   2%|▏         | 30/2000 [00:10<09:44,  3.37it/s, Loss=1.4598874, Gaussian number=182686, print grad=0.002598752733319998, Depth Loss=0.0000000]
Training progress:   2%|▏         | 30/2000 [00:12<09:44,  3.37it/s, Loss=1.5733670, Gaussian number=182686, print grad=0.0036273770965635777, Depth Loss=0.0000000]
Training progress:   2%|▏         | 40/2000 [00:12<08:01,  4.07it/s, Loss=1.5733670, Gaussian number=182686, print grad=0.0036273770965635777, Depth Loss=0.0000000]
Training progress:   2%|▏         | 40/2000 [00:14<08:01,  4.07it/s, Loss=1.1989516, Gaussian number=182686, print grad=0.004420204553753138, Depth Loss=0.0000000] 
Training progress:   2%|▎         | 50/2000 [00:14<07:04,  4.59it/s, Loss=1.1989516, Gaussian number=182686, print grad=0.004420204553753138, Depth Loss=0.0000000]
Training progress:   2%|▎         | 50/2000 [00:15<07:04,  4.59it/s, Loss=1.3770886, Gaussian number=182686, print grad=0.005657398607581854, Depth Loss=0.0000000]
Training progress:   3%|▎         | 60/2000 [00:15<06:28,  4.99it/s, Loss=1.3770886, Gaussian number=182686, print grad=0.005657398607581854, Depth Loss=0.0000000]
Training progress:   3%|▎         | 60/2000 [00:17<06:28,  4.99it/s, Loss=1.2407481, Gaussian number=182686, print grad=0.007256747223436832, Depth Loss=0.0000000]
Training progress:   4%|▎         | 70/2000 [00:17<06:06,  5.27it/s, Loss=1.2407481, Gaussian number=182686, print grad=0.007256747223436832, Depth Loss=0.0000000]
Training progress:   4%|▎         | 70/2000 [00:19<06:06,  5.27it/s, Loss=1.4364282, Gaussian number=182686, print grad=0.008419116027653217, Depth Loss=0.0000000]
Training progress:   4%|▍         | 80/2000 [00:19<05:50,  5.48it/s, Loss=1.4364282, Gaussian number=182686, print grad=0.008419116027653217, Depth Loss=0.0000000]
Training progress:   4%|▍         | 80/2000 [00:20<05:50,  5.48it/s, Loss=1.2352650, Gaussian number=182686, print grad=0.009623565711081028, Depth Loss=0.0000000]
Training progress:   4%|▍         | 90/2000 [00:20<05:39,  5.63it/s, Loss=1.2352650, Gaussian number=182686, print grad=0.009623565711081028, Depth Loss=0.0000000]
Training progress:   4%|▍         | 90/2000 [00:22<05:39,  5.63it/s, Loss=1.2352694, Gaussian number=182686, print grad=0.011081750504672527, Depth Loss=0.0000000]
Training progress:   5%|▌         | 100/2000 [00:22<05:30,  5.74it/s, Loss=1.2352694, Gaussian number=182686, print grad=0.011081750504672527, Depth Loss=0.0000000]
Training progress:   5%|▌         | 100/2000 [00:26<05:30,  5.74it/s, Loss=1.4436665, Gaussian number=182686, print grad=0.012693374417722225, Depth Loss=0.0000000]
Training progress:   6%|▌         | 110/2000 [00:26<07:25,  4.24it/s, Loss=1.4436665, Gaussian number=182686, print grad=0.012693374417722225, Depth Loss=0.0000000]
Training progress:   6%|▌         | 110/2000 [00:27<07:25,  4.24it/s, Loss=1.1976769, Gaussian number=182686, print grad=0.014255128800868988, Depth Loss=0.0000000]
Training progress:   6%|▌         | 120/2000 [00:27<06:43,  4.65it/s, Loss=1.1976769, Gaussian number=182686, print grad=0.014255128800868988, Depth Loss=0.0000000]
Training progress:   6%|▌         | 120/2000 [00:29<06:43,  4.65it/s, Loss=1.3974447, Gaussian number=182686, print grad=0.016258426010608673, Depth Loss=0.0000000]
Training progress:   6%|▋         | 130/2000 [00:29<06:14,  5.00it/s, Loss=1.3974447, Gaussian number=182686, print grad=0.016258426010608673, Depth Loss=0.0000000]
Training progress:   6%|▋         | 130/2000 [00:31<06:14,  5.00it/s, Loss=1.2685915, Gaussian number=182686, print grad=0.018291408196091652, Depth Loss=0.0000000]
Training progress:   7%|▋         | 140/2000 [00:31<05:53,  5.27it/s, Loss=1.2685915, Gaussian number=182686, print grad=0.018291408196091652, Depth Loss=0.0000000]
Training progress:   7%|▋         | 140/2000 [00:33<05:53,  5.27it/s, Loss=1.0999567, Gaussian number=182686, print grad=0.02007974125444889, Depth Loss=0.0000000] 
Training progress:   8%|▊         | 150/2000 [00:33<05:47,  5.33it/s, Loss=1.0999567, Gaussian number=182686, print grad=0.02007974125444889, Depth Loss=0.0000000]
Training progress:   8%|▊         | 150/2000 [00:34<05:47,  5.33it/s, Loss=1.1736623, Gaussian number=182686, print grad=0.022468557581305504, Depth Loss=0.0000000]
Training progress:   8%|▊         | 160/2000 [00:34<05:40,  5.40it/s, Loss=1.1736623, Gaussian number=182686, print grad=0.022468557581305504, Depth Loss=0.0000000]
Training progress:   8%|▊         | 160/2000 [00:36<05:40,  5.40it/s, Loss=1.1604090, Gaussian number=182686, print grad=0.024197319522500038, Depth Loss=0.0000000]
Training progress:   8%|▊         | 170/2000 [00:36<05:32,  5.51it/s, Loss=1.1604090, Gaussian number=182686, print grad=0.024197319522500038, Depth Loss=0.0000000]
Training progress:   8%|▊         | 170/2000 [00:38<05:32,  5.51it/s, Loss=1.0218390, Gaussian number=182686, print grad=0.02634870447218418, Depth Loss=0.0000000] 
Training progress:   9%|▉         | 180/2000 [00:38<05:24,  5.60it/s, Loss=1.0218390, Gaussian number=182686, print grad=0.02634870447218418, Depth Loss=0.0000000]
Training progress:   9%|▉         | 180/2000 [00:40<05:24,  5.60it/s, Loss=1.2875474, Gaussian number=182686, print grad=0.02812841348350048, Depth Loss=0.0000000]
Training progress:  10%|▉         | 190/2000 [00:40<05:21,  5.63it/s, Loss=1.2875474, Gaussian number=182686, print grad=0.02812841348350048, Depth Loss=0.0000000]
Training progress:  10%|▉         | 190/2000 [00:41<05:21,  5.63it/s, Loss=1.1566518, Gaussian number=182686, print grad=0.030352234840393066, Depth Loss=0.0000000]
Training progress:  10%|█         | 200/2000 [00:41<05:19,  5.64it/s, Loss=1.1566518, Gaussian number=182686, print grad=0.030352234840393066, Depth Loss=0.0000000]
Training progress:  10%|█         | 200/2000 [00:45<05:19,  5.64it/s, Loss=1.2831778, Gaussian number=182686, print grad=0.03273819014430046, Depth Loss=0.0000000] 
Training progress:  10%|█         | 210/2000 [00:45<07:09,  4.17it/s, Loss=1.2831778, Gaussian number=182686, print grad=0.03273819014430046, Depth Loss=0.0000000]
Training progress:  10%|█         | 210/2000 [00:47<07:09,  4.17it/s, Loss=1.0722985, Gaussian number=182686, print grad=0.03482435643672943, Depth Loss=0.0000000]
Training progress:  11%|█         | 220/2000 [00:47<06:27,  4.59it/s, Loss=1.0722985, Gaussian number=182686, print grad=0.03482435643672943, Depth Loss=0.0000000]
Training progress:  11%|█         | 220/2000 [00:49<06:27,  4.59it/s, Loss=1.1850899, Gaussian number=182686, print grad=0.036958687007427216, Depth Loss=0.0000000]
Training progress:  12%|█▏        | 230/2000 [00:49<05:59,  4.93it/s, Loss=1.1850899, Gaussian number=182686, print grad=0.036958687007427216, Depth Loss=0.0000000]
Training progress:  12%|█▏        | 230/2000 [00:50<05:59,  4.93it/s, Loss=1.4236308, Gaussian number=182686, print grad=0.0390387699007988, Depth Loss=0.0000000]  
Training progress:  12%|█▏        | 240/2000 [00:50<05:38,  5.20it/s, Loss=1.4236308, Gaussian number=182686, print grad=0.0390387699007988, Depth Loss=0.0000000]
Training progress:  12%|█▏        | 240/2000 [00:52<05:38,  5.20it/s, Loss=1.0948706, Gaussian number=182686, print grad=0.04143015295267105, Depth Loss=0.0000000]
Training progress:  12%|█▎        | 250/2000 [00:52<05:24,  5.39it/s, Loss=1.0948706, Gaussian number=182686, print grad=0.04143015295267105, Depth Loss=0.0000000]
Training progress:  12%|█▎        | 250/2000 [00:54<05:24,  5.39it/s, Loss=1.2330509, Gaussian number=182686, print grad=0.04364221543073654, Depth Loss=0.0000000]
Training progress:  13%|█▎        | 260/2000 [00:54<05:14,  5.53it/s, Loss=1.2330509, Gaussian number=182686, print grad=0.04364221543073654, Depth Loss=0.0000000]
Training progress:  13%|█▎        | 260/2000 [00:55<05:14,  5.53it/s, Loss=0.9100403, Gaussian number=182686, print grad=0.04595113918185234, Depth Loss=0.0000000]
Training progress:  14%|█▎        | 270/2000 [00:55<05:05,  5.66it/s, Loss=0.9100403, Gaussian number=182686, print grad=0.04595113918185234, Depth Loss=0.0000000]
Training progress:  14%|█▎        | 270/2000 [00:57<05:05,  5.66it/s, Loss=1.1751018, Gaussian number=182686, print grad=0.048801008611917496, Depth Loss=0.0000000]
Training progress:  14%|█▍        | 280/2000 [00:57<04:59,  5.74it/s, Loss=1.1751018, Gaussian number=182686, print grad=0.048801008611917496, Depth Loss=0.0000000]
Training progress:  14%|█▍        | 280/2000 [00:59<04:59,  5.74it/s, Loss=1.1426055, Gaussian number=182686, print grad=0.051312923431396484, Depth Loss=0.0000000]
Training progress:  14%|█▍        | 290/2000 [00:59<04:54,  5.82it/s, Loss=1.1426055, Gaussian number=182686, print grad=0.051312923431396484, Depth Loss=0.0000000]
Training progress:  14%|█▍        | 290/2000 [01:00<04:54,  5.82it/s, Loss=1.0642591, Gaussian number=182686, print grad=0.05381336063146591, Depth Loss=0.0000000] 
Training progress:  15%|█▌        | 300/2000 [01:00<04:49,  5.86it/s, Loss=1.0642591, Gaussian number=182686, print grad=0.05381336063146591, Depth Loss=0.0000000]
Training progress:  15%|█▌        | 300/2000 [01:04<04:49,  5.86it/s, Loss=0.9067575, Gaussian number=182686, print grad=0.056641001254320145, Depth Loss=0.0000000]
Training progress:  16%|█▌        | 310/2000 [01:04<06:35,  4.28it/s, Loss=0.9067575, Gaussian number=182686, print grad=0.056641001254320145, Depth Loss=0.0000000]
Training progress:  16%|█▌        | 310/2000 [01:06<06:35,  4.28it/s, Loss=0.9416505, Gaussian number=182686, print grad=0.05851757153868675, Depth Loss=0.0000000] 
Training progress:  16%|█▌        | 320/2000 [01:06<05:59,  4.67it/s, Loss=0.9416505, Gaussian number=182686, print grad=0.05851757153868675, Depth Loss=0.0000000]
Training progress:  16%|█▌        | 320/2000 [01:07<05:59,  4.67it/s, Loss=1.2219106, Gaussian number=182686, print grad=0.06095082685351372, Depth Loss=0.0000000]
Training progress:  16%|█▋        | 330/2000 [01:07<05:33,  5.01it/s, Loss=1.2219106, Gaussian number=182686, print grad=0.06095082685351372, Depth Loss=0.0000000]
Training progress:  16%|█▋        | 330/2000 [01:09<05:33,  5.01it/s, Loss=0.9204277, Gaussian number=182686, print grad=0.0636354312300682, Depth Loss=0.0000000] 
Training progress:  17%|█▋        | 340/2000 [01:09<05:14,  5.27it/s, Loss=0.9204277, Gaussian number=182686, print grad=0.0636354312300682, Depth Loss=0.0000000]
Training progress:  17%|█▋        | 340/2000 [01:11<05:14,  5.27it/s, Loss=0.9479062, Gaussian number=182686, print grad=0.06622511893510818, Depth Loss=0.0000000]
Training progress:  18%|█▊        | 350/2000 [01:11<05:00,  5.48it/s, Loss=0.9479062, Gaussian number=182686, print grad=0.06622511893510818, Depth Loss=0.0000000]
Training progress:  18%|█▊        | 350/2000 [01:12<05:00,  5.48it/s, Loss=0.9052442, Gaussian number=182686, print grad=0.06936314702033997, Depth Loss=0.0000000]
Training progress:  18%|█▊        | 360/2000 [01:12<04:50,  5.64it/s, Loss=0.9052442, Gaussian number=182686, print grad=0.06936314702033997, Depth Loss=0.0000000]
Training progress:  18%|█▊        | 360/2000 [01:14<04:50,  5.64it/s, Loss=0.9252760, Gaussian number=182686, print grad=0.0721343532204628, Depth Loss=0.0000000] 
Training progress:  18%|█▊        | 370/2000 [01:14<04:45,  5.71it/s, Loss=0.9252760, Gaussian number=182686, print grad=0.0721343532204628, Depth Loss=0.0000000]
Training progress:  18%|█▊        | 370/2000 [01:16<04:45,  5.71it/s, Loss=1.1791326, Gaussian number=182686, print grad=0.07434310764074326, Depth Loss=0.0000000]
Training progress:  19%|█▉        | 380/2000 [01:16<04:40,  5.78it/s, Loss=1.1791326, Gaussian number=182686, print grad=0.07434310764074326, Depth Loss=0.0000000]
Training progress:  19%|█▉        | 380/2000 [01:17<04:40,  5.78it/s, Loss=1.0836542, Gaussian number=182686, print grad=0.07706806808710098, Depth Loss=0.0000000]
Training progress:  20%|█▉        | 390/2000 [01:17<04:34,  5.86it/s, Loss=1.0836542, Gaussian number=182686, print grad=0.07706806808710098, Depth Loss=0.0000000]
Training progress:  20%|█▉        | 390/2000 [01:19<04:34,  5.86it/s, Loss=1.2684818, Gaussian number=182686, print grad=0.07956506311893463, Depth Loss=0.0000000]
Training progress:  20%|██        | 400/2000 [01:19<04:31,  5.89it/s, Loss=1.2684818, Gaussian number=182686, print grad=0.07956506311893463, Depth Loss=0.0000000]
Training progress:  20%|██        | 400/2000 [01:23<04:31,  5.89it/s, Loss=1.0860395, Gaussian number=182686, print grad=0.08270977437496185, Depth Loss=0.0000000]
Training progress:  20%|██        | 410/2000 [01:23<06:08,  4.31it/s, Loss=1.0860395, Gaussian number=182686, print grad=0.08270977437496185, Depth Loss=0.0000000]
Training progress:  20%|██        | 410/2000 [01:25<06:08,  4.31it/s, Loss=0.9738409, Gaussian number=182686, print grad=0.0856405571103096, Depth Loss=0.0000000] 
Training progress:  21%|██        | 420/2000 [01:25<05:34,  4.72it/s, Loss=0.9738409, Gaussian number=182686, print grad=0.0856405571103096, Depth Loss=0.0000000]
Training progress:  21%|██        | 420/2000 [01:26<05:34,  4.72it/s, Loss=1.2087361, Gaussian number=182686, print grad=0.08867740631103516, Depth Loss=0.0000000]
Training progress:  22%|██▏       | 430/2000 [01:26<05:11,  5.05it/s, Loss=1.2087361, Gaussian number=182686, print grad=0.08867740631103516, Depth Loss=0.0000000]
Training progress:  22%|██▏       | 430/2000 [01:28<05:11,  5.05it/s, Loss=0.9591514, Gaussian number=182686, print grad=0.09152668714523315, Depth Loss=0.0000000]
Training progress:  22%|██▏       | 440/2000 [01:28<04:54,  5.30it/s, Loss=0.9591514, Gaussian number=182686, print grad=0.09152668714523315, Depth Loss=0.0000000]
Training progress:  22%|██▏       | 440/2000 [01:30<04:54,  5.30it/s, Loss=1.0643859, Gaussian number=182686, print grad=0.09456537663936615, Depth Loss=0.0000000]
Training progress:  22%|██▎       | 450/2000 [01:30<04:41,  5.51it/s, Loss=1.0643859, Gaussian number=182686, print grad=0.09456537663936615, Depth Loss=0.0000000]
Training progress:  22%|██▎       | 450/2000 [01:31<04:41,  5.51it/s, Loss=1.0790797, Gaussian number=182686, print grad=0.09745646268129349, Depth Loss=0.0000000]
Training progress:  23%|██▎       | 460/2000 [01:31<04:31,  5.67it/s, Loss=1.0790797, Gaussian number=182686, print grad=0.09745646268129349, Depth Loss=0.0000000]
Training progress:  23%|██▎       | 460/2000 [01:33<04:31,  5.67it/s, Loss=1.2978999, Gaussian number=182686, print grad=0.10014073550701141, Depth Loss=0.0000000]
Training progress:  24%|██▎       | 470/2000 [01:33<04:24,  5.79it/s, Loss=1.2978999, Gaussian number=182686, print grad=0.10014073550701141, Depth Loss=0.0000000]
Training progress:  24%|██▎       | 470/2000 [01:34<04:24,  5.79it/s, Loss=0.8840409, Gaussian number=182686, print grad=0.10321272909641266, Depth Loss=0.0000000]
Training progress:  24%|██▍       | 480/2000 [01:34<04:19,  5.85it/s, Loss=0.8840409, Gaussian number=182686, print grad=0.10321272909641266, Depth Loss=0.0000000]
Training progress:  24%|██▍       | 480/2000 [01:36<04:19,  5.85it/s, Loss=0.9383316, Gaussian number=182686, print grad=0.10603821277618408, Depth Loss=0.0000000]
Training progress:  24%|██▍       | 490/2000 [01:36<04:14,  5.93it/s, Loss=0.9383316, Gaussian number=182686, print grad=0.10603821277618408, Depth Loss=0.0000000]
Training progress:  24%|██▍       | 490/2000 [01:38<04:14,  5.93it/s, Loss=0.7685703, Gaussian number=182686, print grad=0.10890785604715347, Depth Loss=0.0000000]
Training progress:  25%|██▌       | 500/2000 [01:38<04:10,  5.99it/s, Loss=0.7685703, Gaussian number=182686, print grad=0.10890785604715347, Depth Loss=0.0000000]
Training progress:  25%|██▌       | 500/2000 [01:41<04:10,  5.99it/s, Loss=0.8855236, Gaussian number=182686, print grad=0.111862912774086, Depth Loss=0.0000000]  
Training progress:  26%|██▌       | 510/2000 [01:41<05:39,  4.38it/s, Loss=0.8855236, Gaussian number=182686, print grad=0.111862912774086, Depth Loss=0.0000000]
Training progress:  26%|██▌       | 510/2000 [01:43<05:39,  4.38it/s, Loss=0.9085469, Gaussian number=182686, print grad=0.11497810482978821, Depth Loss=0.0000000]
Training progress:  26%|██▌       | 520/2000 [01:43<05:09,  4.79it/s, Loss=0.9085469, Gaussian number=182686, print grad=0.11497810482978821, Depth Loss=0.0000000]
Training progress:  26%|██▌       | 520/2000 [01:45<05:09,  4.79it/s, Loss=0.7425559, Gaussian number=182686, print grad=0.11759798228740692, Depth Loss=0.0000000]
Training progress:  26%|██▋       | 530/2000 [01:45<04:46,  5.14it/s, Loss=0.7425559, Gaussian number=182686, print grad=0.11759798228740692, Depth Loss=0.0000000]
Training progress:  26%|██▋       | 530/2000 [01:46<04:46,  5.14it/s, Loss=0.9963572, Gaussian number=182686, print grad=0.12051857262849808, Depth Loss=0.0000000]
Training progress:  27%|██▋       | 540/2000 [01:46<04:30,  5.39it/s, Loss=0.9963572, Gaussian number=182686, print grad=0.12051857262849808, Depth Loss=0.0000000]
Training progress:  27%|██▋       | 540/2000 [01:48<04:30,  5.39it/s, Loss=0.9530308, Gaussian number=182686, print grad=0.12367773056030273, Depth Loss=0.0000000]
Training progress:  28%|██▊       | 550/2000 [01:48<04:19,  5.59it/s, Loss=0.9530308, Gaussian number=182686, print grad=0.12367773056030273, Depth Loss=0.0000000]
Training progress:  28%|██▊       | 550/2000 [01:50<04:19,  5.59it/s, Loss=0.7760994, Gaussian number=182686, print grad=0.12670420110225677, Depth Loss=0.0000000]
Training progress:  28%|██▊       | 560/2000 [01:50<04:12,  5.71it/s, Loss=0.7760994, Gaussian number=182686, print grad=0.12670420110225677, Depth Loss=0.0000000]
Training progress:  28%|██▊       | 560/2000 [01:51<04:12,  5.71it/s, Loss=0.9894730, Gaussian number=182686, print grad=0.13013555109500885, Depth Loss=0.0000000]
Training progress:  28%|██▊       | 570/2000 [01:51<04:06,  5.81it/s, Loss=0.9894730, Gaussian number=182686, print grad=0.13013555109500885, Depth Loss=0.0000000]
Training progress:  28%|██▊       | 570/2000 [01:53<04:06,  5.81it/s, Loss=0.8575344, Gaussian number=182686, print grad=0.1331392526626587, Depth Loss=0.0000000] 
Training progress:  29%|██▉       | 580/2000 [01:53<04:02,  5.86it/s, Loss=0.8575344, Gaussian number=182686, print grad=0.1331392526626587, Depth Loss=0.0000000]
Training progress:  29%|██▉       | 580/2000 [01:55<04:02,  5.86it/s, Loss=0.9689923, Gaussian number=182686, print grad=0.1363048255443573, Depth Loss=0.0000000]
Training progress:  30%|██▉       | 590/2000 [01:55<03:58,  5.90it/s, Loss=0.9689923, Gaussian number=182686, print grad=0.1363048255443573, Depth Loss=0.0000000]
Training progress:  30%|██▉       | 590/2000 [01:56<03:58,  5.90it/s, Loss=0.9963781, Gaussian number=182686, print grad=0.13923615217208862, Depth Loss=0.0000000]
Training progress:  30%|███       | 600/2000 [01:56<03:55,  5.95it/s, Loss=0.9963781, Gaussian number=182686, print grad=0.13923615217208862, Depth Loss=0.0000000]
Training progress:  30%|███       | 600/2000 [02:01<03:55,  5.95it/s, Loss=0.9901307, Gaussian number=270280, print grad=0.002430034801363945, Depth Loss=0.0000000]
Training progress:  30%|███       | 610/2000 [02:01<05:50,  3.96it/s, Loss=0.9901307, Gaussian number=270280, print grad=0.002430034801363945, Depth Loss=0.0000000]
Training progress:  30%|███       | 610/2000 [02:02<05:50,  3.96it/s, Loss=1.2420546, Gaussian number=270280, print grad=0.00547967990860343, Depth Loss=0.0000000] 
Training progress:  31%|███       | 620/2000 [02:02<05:11,  4.43it/s, Loss=1.2420546, Gaussian number=270280, print grad=0.00547967990860343, Depth Loss=0.0000000]
Training progress:  31%|███       | 620/2000 [02:04<05:11,  4.43it/s, Loss=0.9254725, Gaussian number=270280, print grad=0.007801834028214216, Depth Loss=0.0000000]
Training progress:  32%|███▏      | 630/2000 [02:04<04:46,  4.79it/s, Loss=0.9254725, Gaussian number=270280, print grad=0.007801834028214216, Depth Loss=0.0000000]
Training progress:  32%|███▏      | 630/2000 [02:06<04:46,  4.79it/s, Loss=0.9804040, Gaussian number=270280, print grad=0.010949883610010147, Depth Loss=0.0000000]
Training progress:  32%|███▏      | 640/2000 [02:06<04:27,  5.09it/s, Loss=0.9804040, Gaussian number=270280, print grad=0.010949883610010147, Depth Loss=0.0000000]
Training progress:  32%|███▏      | 640/2000 [02:07<04:27,  5.09it/s, Loss=1.0584496, Gaussian number=270280, print grad=0.013136155903339386, Depth Loss=0.0000000]
Training progress:  32%|███▎      | 650/2000 [02:07<04:12,  5.34it/s, Loss=1.0584496, Gaussian number=270280, print grad=0.013136155903339386, Depth Loss=0.0000000]
Training progress:  32%|███▎      | 650/2000 [02:09<04:12,  5.34it/s, Loss=1.0897983, Gaussian number=270280, print grad=0.01579233817756176, Depth Loss=0.0000000] 
Training progress:  33%|███▎      | 660/2000 [02:09<04:02,  5.52it/s, Loss=1.0897983, Gaussian number=270280, print grad=0.01579233817756176, Depth Loss=0.0000000]
Training progress:  33%|███▎      | 660/2000 [02:11<04:02,  5.52it/s, Loss=0.9265104, Gaussian number=270280, print grad=0.01823960244655609, Depth Loss=0.0000000]
Training progress:  34%|███▎      | 670/2000 [02:11<03:54,  5.67it/s, Loss=0.9265104, Gaussian number=270280, print grad=0.01823960244655609, Depth Loss=0.0000000]
Training progress:  34%|███▎      | 670/2000 [02:12<03:54,  5.67it/s, Loss=0.8674375, Gaussian number=270280, print grad=0.02103782445192337, Depth Loss=0.0000000]
Training progress:  34%|███▍      | 680/2000 [02:12<03:48,  5.78it/s, Loss=0.8674375, Gaussian number=270280, print grad=0.02103782445192337, Depth Loss=0.0000000]
Training progress:  34%|███▍      | 680/2000 [02:14<03:48,  5.78it/s, Loss=0.9997780, Gaussian number=270280, print grad=0.023481514304876328, Depth Loss=0.0000000]
Training progress:  34%|███▍      | 690/2000 [02:14<03:43,  5.85it/s, Loss=0.9997780, Gaussian number=270280, print grad=0.023481514304876328, Depth Loss=0.0000000]
Training progress:  34%|███▍      | 690/2000 [02:16<03:43,  5.85it/s, Loss=0.9917889, Gaussian number=270280, print grad=0.02584807388484478, Depth Loss=0.0000000] 
Training progress:  35%|███▌      | 700/2000 [02:16<03:39,  5.91it/s, Loss=0.9917889, Gaussian number=270280, print grad=0.02584807388484478, Depth Loss=0.0000000]
Training progress:  35%|███▌      | 700/2000 [02:20<03:39,  5.91it/s, Loss=0.9914882, Gaussian number=400305, print grad=0.0017300993204116821, Depth Loss=0.0000000]
Training progress:  36%|███▌      | 710/2000 [02:20<04:59,  4.31it/s, Loss=0.9914882, Gaussian number=400305, print grad=0.0017300993204116821, Depth Loss=0.0000000]
Training progress:  36%|███▌      | 710/2000 [02:21<04:59,  4.31it/s, Loss=0.8879708, Gaussian number=400305, print grad=0.0038955502677708864, Depth Loss=0.0000000]
Training progress:  36%|███▌      | 720/2000 [02:21<04:31,  4.72it/s, Loss=0.8879708, Gaussian number=400305, print grad=0.0038955502677708864, Depth Loss=0.0000000]
Training progress:  36%|███▌      | 720/2000 [02:23<04:31,  4.72it/s, Loss=1.0887761, Gaussian number=400305, print grad=0.005735951010137796, Depth Loss=0.0000000] 
Training progress:  36%|███▋      | 730/2000 [02:23<04:10,  5.06it/s, Loss=1.0887761, Gaussian number=400305, print grad=0.005735951010137796, Depth Loss=0.0000000]
Training progress:  36%|███▋      | 730/2000 [02:24<04:10,  5.06it/s, Loss=1.3199967, Gaussian number=400305, print grad=0.008064514957368374, Depth Loss=0.0000000]
Training progress:  37%|███▋      | 740/2000 [02:24<03:56,  5.33it/s, Loss=1.3199967, Gaussian number=400305, print grad=0.008064514957368374, Depth Loss=0.0000000]
Training progress:  37%|███▋      | 740/2000 [02:26<03:56,  5.33it/s, Loss=0.9381611, Gaussian number=400305, print grad=0.010219226591289043, Depth Loss=0.0000000]
Training progress:  38%|███▊      | 750/2000 [02:26<03:46,  5.51it/s, Loss=0.9381611, Gaussian number=400305, print grad=0.010219226591289043, Depth Loss=0.0000000]
Training progress:  38%|███▊      | 750/2000 [02:28<03:46,  5.51it/s, Loss=0.8893908, Gaussian number=400305, print grad=0.01224812027066946, Depth Loss=0.0000000] 
Training progress:  38%|███▊      | 760/2000 [02:28<03:39,  5.65it/s, Loss=0.8893908, Gaussian number=400305, print grad=0.01224812027066946, Depth Loss=0.0000000]
Training progress:  38%|███▊      | 760/2000 [02:29<03:39,  5.65it/s, Loss=0.8016575, Gaussian number=400305, print grad=0.014293739572167397, Depth Loss=0.0000000]
Training progress:  38%|███▊      | 770/2000 [02:29<03:33,  5.77it/s, Loss=0.8016575, Gaussian number=400305, print grad=0.014293739572167397, Depth Loss=0.0000000]
Training progress:  38%|███▊      | 770/2000 [02:31<03:33,  5.77it/s, Loss=1.0420634, Gaussian number=400305, print grad=0.01622144691646099, Depth Loss=0.0000000] 
Training progress:  39%|███▉      | 780/2000 [02:31<03:28,  5.85it/s, Loss=1.0420634, Gaussian number=400305, print grad=0.01622144691646099, Depth Loss=0.0000000]
Training progress:  39%|███▉      | 780/2000 [02:33<03:28,  5.85it/s, Loss=1.1772294, Gaussian number=400305, print grad=0.018171312287449837, Depth Loss=0.0000000]
Training progress:  40%|███▉      | 790/2000 [02:33<03:25,  5.90it/s, Loss=1.1772294, Gaussian number=400305, print grad=0.018171312287449837, Depth Loss=0.0000000]
Training progress:  40%|███▉      | 790/2000 [02:34<03:25,  5.90it/s, Loss=0.9839810, Gaussian number=400305, print grad=0.020338211208581924, Depth Loss=0.0000000]
Training progress:  40%|████      | 800/2000 [02:34<03:22,  5.93it/s, Loss=0.9839810, Gaussian number=400305, print grad=0.020338211208581924, Depth Loss=0.0000000]
Training progress:  40%|████      | 800/2000 [02:38<03:22,  5.93it/s, Loss=1.1972564, Gaussian number=554666, print grad=0.0016476210439577699, Depth Loss=0.0000000]
Training progress:  40%|████      | 810/2000 [02:38<04:36,  4.31it/s, Loss=1.1972564, Gaussian number=554666, print grad=0.0016476210439577699, Depth Loss=0.0000000]
Training progress:  40%|████      | 810/2000 [02:40<04:36,  4.31it/s, Loss=1.0785886, Gaussian number=554666, print grad=0.003296273062005639, Depth Loss=0.0000000] 
Training progress:  41%|████      | 820/2000 [02:40<04:11,  4.70it/s, Loss=1.0785886, Gaussian number=554666, print grad=0.003296273062005639, Depth Loss=0.0000000]
Training progress:  41%|████      | 820/2000 [02:42<04:11,  4.70it/s, Loss=0.9068370, Gaussian number=554666, print grad=0.005336198955774307, Depth Loss=0.0000000]
Training progress:  42%|████▏     | 830/2000 [02:42<03:52,  5.03it/s, Loss=0.9068370, Gaussian number=554666, print grad=0.005336198955774307, Depth Loss=0.0000000]
Training progress:  42%|████▏     | 830/2000 [02:43<03:52,  5.03it/s, Loss=0.8954681, Gaussian number=554666, print grad=0.0070474278181791306, Depth Loss=0.0000000]
Training progress:  42%|████▏     | 840/2000 [02:43<03:39,  5.29it/s, Loss=0.8954681, Gaussian number=554666, print grad=0.0070474278181791306, Depth Loss=0.0000000]
Training progress:  42%|████▏     | 840/2000 [02:45<03:39,  5.29it/s, Loss=0.9332988, Gaussian number=554666, print grad=0.008850649930536747, Depth Loss=0.0000000] 
Training progress:  42%|████▎     | 850/2000 [02:45<03:29,  5.49it/s, Loss=0.9332988, Gaussian number=554666, print grad=0.008850649930536747, Depth Loss=0.0000000]
Training progress:  42%|████▎     | 850/2000 [02:47<03:29,  5.49it/s, Loss=0.9076974, Gaussian number=554666, print grad=0.010459132492542267, Depth Loss=0.0000000]
Training progress:  43%|████▎     | 860/2000 [02:47<03:23,  5.61it/s, Loss=0.9076974, Gaussian number=554666, print grad=0.010459132492542267, Depth Loss=0.0000000]
Training progress:  43%|████▎     | 860/2000 [02:48<03:23,  5.61it/s, Loss=1.0439925, Gaussian number=554666, print grad=0.01207699440419674, Depth Loss=0.0000000] 
Training progress:  44%|████▎     | 870/2000 [02:48<03:18,  5.70it/s, Loss=1.0439925, Gaussian number=554666, print grad=0.01207699440419674, Depth Loss=0.0000000]
Training progress:  44%|████▎     | 870/2000 [02:50<03:18,  5.70it/s, Loss=0.9954019, Gaussian number=554666, print grad=0.013627083040773869, Depth Loss=0.0000000]
Training progress:  44%|████▍     | 880/2000 [02:50<03:14,  5.77it/s, Loss=0.9954019, Gaussian number=554666, print grad=0.013627083040773869, Depth Loss=0.0000000]
Training progress:  44%|████▍     | 880/2000 [02:52<03:14,  5.77it/s, Loss=0.7708614, Gaussian number=554666, print grad=0.015384571626782417, Depth Loss=0.0000000]
Training progress:  44%|████▍     | 890/2000 [02:52<03:10,  5.81it/s, Loss=0.7708614, Gaussian number=554666, print grad=0.015384571626782417, Depth Loss=0.0000000]
Training progress:  44%|████▍     | 890/2000 [02:53<03:10,  5.81it/s, Loss=0.9792877, Gaussian number=554666, print grad=0.016907358542084694, Depth Loss=0.0000000]
Training progress:  45%|████▌     | 900/2000 [02:53<03:07,  5.86it/s, Loss=0.9792877, Gaussian number=554666, print grad=0.016907358542084694, Depth Loss=0.0000000]
Training progress:  45%|████▌     | 900/2000 [02:57<03:07,  5.86it/s, Loss=0.9284445, Gaussian number=740903, print grad=0.0013603916158899665, Depth Loss=0.0000000]
Training progress:  46%|████▌     | 910/2000 [02:57<04:16,  4.25it/s, Loss=0.9284445, Gaussian number=740903, print grad=0.0013603916158899665, Depth Loss=0.0000000]
Training progress:  46%|████▌     | 910/2000 [02:59<04:16,  4.25it/s, Loss=1.0465210, Gaussian number=740903, print grad=0.002559632994234562, Depth Loss=0.0000000] 
Training progress:  46%|████▌     | 920/2000 [02:59<03:52,  4.64it/s, Loss=1.0465210, Gaussian number=740903, print grad=0.002559632994234562, Depth Loss=0.0000000]
Training progress:  46%|████▌     | 920/2000 [03:01<03:52,  4.64it/s, Loss=1.1123435, Gaussian number=740903, print grad=0.004222213290631771, Depth Loss=0.0000000]
Training progress:  46%|████▋     | 930/2000 [03:01<03:34,  4.98it/s, Loss=1.1123435, Gaussian number=740903, print grad=0.004222213290631771, Depth Loss=0.0000000]
Training progress:  46%|████▋     | 930/2000 [03:02<03:34,  4.98it/s, Loss=0.9691876, Gaussian number=740903, print grad=0.005579343065619469, Depth Loss=0.0000000]
Training progress:  47%|████▋     | 940/2000 [03:02<03:22,  5.24it/s, Loss=0.9691876, Gaussian number=740903, print grad=0.005579343065619469, Depth Loss=0.0000000]
Training progress:  47%|████▋     | 940/2000 [03:04<03:22,  5.24it/s, Loss=0.9437048, Gaussian number=740903, print grad=0.007043318822979927, Depth Loss=0.0000000]
Training progress:  48%|████▊     | 950/2000 [03:04<03:15,  5.38it/s, Loss=0.9437048, Gaussian number=740903, print grad=0.007043318822979927, Depth Loss=0.0000000]
Training progress:  48%|████▊     | 950/2000 [03:06<03:15,  5.38it/s, Loss=0.9117686, Gaussian number=740903, print grad=0.008390852250158787, Depth Loss=0.0000000]
Training progress:  48%|████▊     | 960/2000 [03:06<03:08,  5.51it/s, Loss=0.9117686, Gaussian number=740903, print grad=0.008390852250158787, Depth Loss=0.0000000]
Training progress:  48%|████▊     | 960/2000 [03:07<03:08,  5.51it/s, Loss=1.1258527, Gaussian number=740903, print grad=0.0098097063601017, Depth Loss=0.0000000]  
Training progress:  48%|████▊     | 970/2000 [03:07<03:03,  5.62it/s, Loss=1.1258527, Gaussian number=740903, print grad=0.0098097063601017, Depth Loss=0.0000000]
Training progress:  48%|████▊     | 970/2000 [03:09<03:03,  5.62it/s, Loss=0.7495213, Gaussian number=740903, print grad=0.011128578335046768, Depth Loss=0.0000000]
Training progress:  49%|████▉     | 980/2000 [03:09<02:58,  5.70it/s, Loss=0.7495213, Gaussian number=740903, print grad=0.011128578335046768, Depth Loss=0.0000000]
Training progress:  49%|████▉     | 980/2000 [03:11<02:58,  5.70it/s, Loss=0.7699706, Gaussian number=740903, print grad=0.012357882224023342, Depth Loss=0.0000000]
Training progress:  50%|████▉     | 990/2000 [03:11<02:54,  5.77it/s, Loss=0.7699706, Gaussian number=740903, print grad=0.012357882224023342, Depth Loss=0.0000000]
Training progress:  50%|████▉     | 990/2000 [03:12<02:54,  5.77it/s, Loss=0.9692752, Gaussian number=740903, print grad=0.013484220951795578, Depth Loss=0.0000000]
Training progress:  50%|█████     | 1000/2000 [03:12<02:51,  5.82it/s, Loss=0.9692752, Gaussian number=740903, print grad=0.013484220951795578, Depth Loss=0.0000000]
Training progress:  50%|█████     | 1000/2000 [03:16<02:51,  5.82it/s, Loss=1.0333875, Gaussian number=956218, print grad=0.0009610269335098565, Depth Loss=0.0000000]
Training progress:  50%|█████     | 1010/2000 [03:16<03:55,  4.20it/s, Loss=1.0333875, Gaussian number=956218, print grad=0.0009610269335098565, Depth Loss=0.0000000]
Training progress:  50%|█████     | 1010/2000 [03:18<03:55,  4.20it/s, Loss=1.2458187, Gaussian number=956218, print grad=0.0023271916434168816, Depth Loss=0.0000000]
Training progress:  51%|█████     | 1020/2000 [03:18<03:33,  4.58it/s, Loss=1.2458187, Gaussian number=956218, print grad=0.0023271916434168816, Depth Loss=0.0000000]
Training progress:  51%|█████     | 1020/2000 [03:20<03:33,  4.58it/s, Loss=1.0956973, Gaussian number=956218, print grad=0.0035282550379633904, Depth Loss=0.0000000]
Training progress:  52%|█████▏    | 1030/2000 [03:20<03:18,  4.89it/s, Loss=1.0956973, Gaussian number=956218, print grad=0.0035282550379633904, Depth Loss=0.0000000]
Training progress:  52%|█████▏    | 1030/2000 [03:22<03:18,  4.89it/s, Loss=1.0464924, Gaussian number=956218, print grad=0.0048601459711790085, Depth Loss=0.0000000]
Training progress:  52%|█████▏    | 1040/2000 [03:22<03:06,  5.14it/s, Loss=1.0464924, Gaussian number=956218, print grad=0.0048601459711790085, Depth Loss=0.0000000]
Training progress:  52%|█████▏    | 1040/2000 [03:23<03:06,  5.14it/s, Loss=0.9349660, Gaussian number=956218, print grad=0.005890943575650454, Depth Loss=0.0000000] 
Training progress:  52%|█████▎    | 1050/2000 [03:23<02:57,  5.34it/s, Loss=0.9349660, Gaussian number=956218, print grad=0.005890943575650454, Depth Loss=0.0000000]
Training progress:  52%|█████▎    | 1050/2000 [03:25<02:57,  5.34it/s, Loss=0.9402491, Gaussian number=956218, print grad=0.007064362056553364, Depth Loss=0.0000000]
Training progress:  53%|█████▎    | 1060/2000 [03:25<02:52,  5.46it/s, Loss=0.9402491, Gaussian number=956218, print grad=0.007064362056553364, Depth Loss=0.0000000]
Training progress:  53%|█████▎    | 1060/2000 [03:27<02:52,  5.46it/s, Loss=0.7740080, Gaussian number=956218, print grad=0.008339562453329563, Depth Loss=0.0000000]
Training progress:  54%|█████▎    | 1070/2000 [03:27<02:46,  5.57it/s, Loss=0.7740080, Gaussian number=956218, print grad=0.008339562453329563, Depth Loss=0.0000000]
Training progress:  54%|█████▎    | 1070/2000 [03:28<02:46,  5.57it/s, Loss=0.7476606, Gaussian number=956218, print grad=0.009391642175614834, Depth Loss=0.0000000]
Training progress:  54%|█████▍    | 1080/2000 [03:28<02:45,  5.57it/s, Loss=0.7476606, Gaussian number=956218, print grad=0.009391642175614834, Depth Loss=0.0000000]
Training progress:  54%|█████▍    | 1080/2000 [03:30<02:45,  5.57it/s, Loss=0.9630379, Gaussian number=956218, print grad=0.010552738793194294, Depth Loss=0.0000000]
Training progress:  55%|█████▍    | 1090/2000 [03:30<02:42,  5.60it/s, Loss=0.9630379, Gaussian number=956218, print grad=0.010552738793194294, Depth Loss=0.0000000]
Training progress:  55%|█████▍    | 1090/2000 [03:32<02:42,  5.60it/s, Loss=0.9817569, Gaussian number=956218, print grad=0.011683842167258263, Depth Loss=0.0000000]
Training progress:  55%|█████▌    | 1100/2000 [03:32<02:38,  5.66it/s, Loss=0.9817569, Gaussian number=956218, print grad=0.011683842167258263, Depth Loss=0.0000000]
Training progress:  55%|█████▌    | 1100/2000 [03:34<02:38,  5.66it/s, Loss=1.0941141, Gaussian number=1203568, print grad=0.0008546744356863201, Depth Loss=0.0000000]
Training progress:  56%|█████▌    | 1110/2000 [03:34<02:37,  5.65it/s, Loss=1.0941141, Gaussian number=1203568, print grad=0.0008546744356863201, Depth Loss=0.0000000]
Training progress:  56%|█████▌    | 1110/2000 [03:36<02:37,  5.65it/s, Loss=1.1420706, Gaussian number=1203568, print grad=0.0018925158074125648, Depth Loss=0.0000000]
Training progress:  56%|█████▌    | 1120/2000 [03:36<02:36,  5.62it/s, Loss=1.1420706, Gaussian number=1203568, print grad=0.0018925158074125648, Depth Loss=0.0000000]
Training progress:  56%|█████▌    | 1120/2000 [03:37<02:36,  5.62it/s, Loss=0.8925164, Gaussian number=1203568, print grad=0.00304034654982388, Depth Loss=0.0000000]  
Training progress:  56%|█████▋    | 1130/2000 [03:37<02:34,  5.65it/s, Loss=0.8925164, Gaussian number=1203568, print grad=0.00304034654982388, Depth Loss=0.0000000]
Training progress:  56%|█████▋    | 1130/2000 [03:39<02:34,  5.65it/s, Loss=1.0047077, Gaussian number=1203568, print grad=0.00410668458789587, Depth Loss=0.0000000]
Training progress:  57%|█████▋    | 1140/2000 [03:39<02:32,  5.66it/s, Loss=1.0047077, Gaussian number=1203568, print grad=0.00410668458789587, Depth Loss=0.0000000]
Training progress:  57%|█████▋    | 1140/2000 [03:41<02:32,  5.66it/s, Loss=0.7827121, Gaussian number=1203568, print grad=0.005251748021692038, Depth Loss=0.0000000]
Training progress:  57%|█████▊    | 1150/2000 [03:41<02:29,  5.68it/s, Loss=0.7827121, Gaussian number=1203568, print grad=0.005251748021692038, Depth Loss=0.0000000]
Training progress:  57%|█████▊    | 1150/2000 [03:43<02:29,  5.68it/s, Loss=0.7980758, Gaussian number=1203568, print grad=0.006156509276479483, Depth Loss=0.0000000]
Training progress:  58%|█████▊    | 1160/2000 [03:43<02:27,  5.70it/s, Loss=0.7980758, Gaussian number=1203568, print grad=0.006156509276479483, Depth Loss=0.0000000]
Training progress:  58%|█████▊    | 1160/2000 [03:44<02:27,  5.70it/s, Loss=0.9381064, Gaussian number=1203568, print grad=0.007132456637918949, Depth Loss=0.0000000]
Training progress:  58%|█████▊    | 1170/2000 [03:44<02:25,  5.70it/s, Loss=0.9381064, Gaussian number=1203568, print grad=0.007132456637918949, Depth Loss=0.0000000]
Training progress:  58%|█████▊    | 1170/2000 [03:46<02:25,  5.70it/s, Loss=0.9364125, Gaussian number=1203568, print grad=0.008149839006364346, Depth Loss=0.0000000]
Training progress:  59%|█████▉    | 1180/2000 [03:46<02:23,  5.72it/s, Loss=0.9364125, Gaussian number=1203568, print grad=0.008149839006364346, Depth Loss=0.0000000]
Training progress:  59%|█████▉    | 1180/2000 [03:48<02:23,  5.72it/s, Loss=0.9295942, Gaussian number=1203568, print grad=0.009216208010911942, Depth Loss=0.0000000]
Training progress:  60%|█████▉    | 1190/2000 [03:48<02:21,  5.72it/s, Loss=0.9295942, Gaussian number=1203568, print grad=0.009216208010911942, Depth Loss=0.0000000]
Training progress:  60%|█████▉    | 1190/2000 [03:49<02:21,  5.72it/s, Loss=1.0214565, Gaussian number=1203568, print grad=0.010092433542013168, Depth Loss=0.0000000]
Training progress:  60%|██████    | 1200/2000 [03:49<02:19,  5.72it/s, Loss=1.0214565, Gaussian number=1203568, print grad=0.010092433542013168, Depth Loss=0.0000000]
Training progress:  60%|██████    | 1200/2000 [03:51<02:19,  5.72it/s, Loss=0.8868406, Gaussian number=1478199, print grad=0.0008200262091122568, Depth Loss=0.0000000]
Training progress:  60%|██████    | 1210/2000 [03:51<02:18,  5.69it/s, Loss=0.8868406, Gaussian number=1478199, print grad=0.0008200262091122568, Depth Loss=0.0000000]
Training progress:  60%|██████    | 1210/2000 [03:53<02:18,  5.69it/s, Loss=0.9064142, Gaussian number=1478199, print grad=0.0018304369878023863, Depth Loss=0.0000000]
Training progress:  61%|██████    | 1220/2000 [03:53<02:17,  5.67it/s, Loss=0.9064142, Gaussian number=1478199, print grad=0.0018304369878023863, Depth Loss=0.0000000]
Training progress:  61%|██████    | 1220/2000 [03:55<02:17,  5.67it/s, Loss=0.8437227, Gaussian number=1478199, print grad=0.002774778287857771, Depth Loss=0.0000000] 
Training progress:  62%|██████▏   | 1230/2000 [03:55<02:15,  5.68it/s, Loss=0.8437227, Gaussian number=1478199, print grad=0.002774778287857771, Depth Loss=0.0000000]
Training progress:  62%|██████▏   | 1230/2000 [03:57<02:15,  5.68it/s, Loss=0.8130501, Gaussian number=1478199, print grad=0.0037035210989415646, Depth Loss=0.0000000]
Training progress:  62%|██████▏   | 1240/2000 [03:57<02:14,  5.66it/s, Loss=0.8130501, Gaussian number=1478199, print grad=0.0037035210989415646, Depth Loss=0.0000000]
Training progress:  62%|██████▏   | 1240/2000 [03:58<02:14,  5.66it/s, Loss=0.7935236, Gaussian number=1478199, print grad=0.00452639814466238, Depth Loss=0.0000000]  
Training progress:  62%|██████▎   | 1250/2000 [03:58<02:12,  5.66it/s, Loss=0.7935236, Gaussian number=1478199, print grad=0.00452639814466238, Depth Loss=0.0000000]
Training progress:  62%|██████▎   | 1250/2000 [04:00<02:12,  5.66it/s, Loss=0.7868590, Gaussian number=1478199, print grad=0.005295500624924898, Depth Loss=0.0000000]
Training progress:  63%|██████▎   | 1260/2000 [04:00<02:10,  5.66it/s, Loss=0.7868590, Gaussian number=1478199, print grad=0.005295500624924898, Depth Loss=0.0000000]
Training progress:  63%|██████▎   | 1260/2000 [04:02<02:10,  5.66it/s, Loss=0.9107175, Gaussian number=1478199, print grad=0.006164568942040205, Depth Loss=0.0000000]
Training progress:  64%|██████▎   | 1270/2000 [04:02<02:08,  5.68it/s, Loss=0.9107175, Gaussian number=1478199, print grad=0.006164568942040205, Depth Loss=0.0000000]
Training progress:  64%|██████▎   | 1270/2000 [04:04<02:08,  5.68it/s, Loss=0.9186283, Gaussian number=1478199, print grad=0.0069395615719258785, Depth Loss=0.0000000]
Training progress:  64%|██████▍   | 1280/2000 [04:04<02:07,  5.65it/s, Loss=0.9186283, Gaussian number=1478199, print grad=0.0069395615719258785, Depth Loss=0.0000000]
Training progress:  64%|██████▍   | 1280/2000 [04:05<02:07,  5.65it/s, Loss=0.7274998, Gaussian number=1478199, print grad=0.007859492674469948, Depth Loss=0.0000000] 
Training progress:  64%|██████▍   | 1290/2000 [04:05<02:05,  5.67it/s, Loss=0.7274998, Gaussian number=1478199, print grad=0.007859492674469948, Depth Loss=0.0000000]
Training progress:  64%|██████▍   | 1290/2000 [04:07<02:05,  5.67it/s, Loss=1.1258148, Gaussian number=1478199, print grad=0.008676921017467976, Depth Loss=0.0000000]
Training progress:  65%|██████▌   | 1300/2000 [04:07<02:03,  5.67it/s, Loss=1.1258148, Gaussian number=1478199, print grad=0.008676921017467976, Depth Loss=0.0000000]
Training progress:  65%|██████▌   | 1300/2000 [04:09<02:03,  5.67it/s, Loss=1.1536823, Gaussian number=1784076, print grad=0.0007993551553227007, Depth Loss=0.0000000]
Training progress:  66%|██████▌   | 1310/2000 [04:09<02:03,  5.60it/s, Loss=1.1536823, Gaussian number=1784076, print grad=0.0007993551553227007, Depth Loss=0.0000000]
Training progress:  66%|██████▌   | 1310/2000 [04:11<02:03,  5.60it/s, Loss=1.1872289, Gaussian number=1784076, print grad=0.0015933553222566843, Depth Loss=0.0000000]
Training progress:  66%|██████▌   | 1320/2000 [04:11<02:02,  5.56it/s, Loss=1.1872289, Gaussian number=1784076, print grad=0.0015933553222566843, Depth Loss=0.0000000]
Training progress:  66%|██████▌   | 1320/2000 [04:13<02:02,  5.56it/s, Loss=0.9856800, Gaussian number=1784076, print grad=0.002492056228220463, Depth Loss=0.0000000] 
Training progress:  66%|██████▋   | 1330/2000 [04:13<02:01,  5.51it/s, Loss=0.9856800, Gaussian number=1784076, print grad=0.002492056228220463, Depth Loss=0.0000000]
Training progress:  66%|██████▋   | 1330/2000 [04:15<02:01,  5.51it/s, Loss=0.9472752, Gaussian number=1784076, print grad=0.0032473914325237274, Depth Loss=0.0000000]
Training progress:  67%|██████▋   | 1340/2000 [04:15<01:59,  5.51it/s, Loss=0.9472752, Gaussian number=1784076, print grad=0.0032473914325237274, Depth Loss=0.0000000]
Training progress:  67%|██████▋   | 1340/2000 [04:16<01:59,  5.51it/s, Loss=1.1898069, Gaussian number=1784076, print grad=0.003912332933396101, Depth Loss=0.0000000] 
Training progress:  68%|██████▊   | 1350/2000 [04:16<01:57,  5.52it/s, Loss=1.1898069, Gaussian number=1784076, print grad=0.003912332933396101, Depth Loss=0.0000000]
Training progress:  68%|██████▊   | 1350/2000 [04:18<01:57,  5.52it/s, Loss=0.7316759, Gaussian number=1784076, print grad=0.004632408265024424, Depth Loss=0.0000000]
Training progress:  68%|██████▊   | 1360/2000 [04:18<01:55,  5.54it/s, Loss=0.7316759, Gaussian number=1784076, print grad=0.004632408265024424, Depth Loss=0.0000000]
Training progress:  68%|██████▊   | 1360/2000 [04:21<01:55,  5.54it/s, Loss=1.3115074, Gaussian number=1784076, print grad=0.0054523637518286705, Depth Loss=0.0000000]
Training progress:  68%|██████▊   | 1370/2000 [04:21<02:16,  4.60it/s, Loss=1.3115074, Gaussian number=1784076, print grad=0.0054523637518286705, Depth Loss=0.0000000]
Training progress:  68%|██████▊   | 1370/2000 [04:23<02:16,  4.60it/s, Loss=0.8905317, Gaussian number=1784076, print grad=0.006128925364464521, Depth Loss=0.0000000] 
Training progress:  69%|██████▉   | 1380/2000 [04:23<02:06,  4.91it/s, Loss=0.8905317, Gaussian number=1784076, print grad=0.006128925364464521, Depth Loss=0.0000000]
Training progress:  69%|██████▉   | 1380/2000 [04:25<02:06,  4.91it/s, Loss=0.7843708, Gaussian number=1784076, print grad=0.006848808843642473, Depth Loss=0.0000000]
Training progress:  70%|██████▉   | 1390/2000 [04:25<01:57,  5.19it/s, Loss=0.7843708, Gaussian number=1784076, print grad=0.006848808843642473, Depth Loss=0.0000000]
Training progress:  70%|██████▉   | 1390/2000 [04:26<01:57,  5.19it/s, Loss=0.8606298, Gaussian number=1784076, print grad=0.007534282747656107, Depth Loss=0.0000000]
Training progress:  70%|███████   | 1400/2000 [04:26<01:50,  5.44it/s, Loss=0.8606298, Gaussian number=1784076, print grad=0.007534282747656107, Depth Loss=0.0000000]
Training progress:  70%|███████   | 1400/2000 [04:28<01:50,  5.44it/s, Loss=1.1168019, Gaussian number=2102226, print grad=0.0006692510796710849, Depth Loss=0.0000000]
Training progress:  70%|███████   | 1410/2000 [04:28<01:45,  5.58it/s, Loss=1.1168019, Gaussian number=2102226, print grad=0.0006692510796710849, Depth Loss=0.0000000]
Training progress:  70%|███████   | 1410/2000 [04:30<01:45,  5.58it/s, Loss=0.9175268, Gaussian number=2102226, print grad=0.001370455021969974, Depth Loss=0.0000000] 
Training progress:  71%|███████   | 1420/2000 [04:30<01:41,  5.70it/s, Loss=0.9175268, Gaussian number=2102226, print grad=0.001370455021969974, Depth Loss=0.0000000]
Training progress:  71%|███████   | 1420/2000 [04:31<01:41,  5.70it/s, Loss=1.0194000, Gaussian number=2102226, print grad=0.002185703255236149, Depth Loss=0.0000000]
Training progress:  72%|███████▏  | 1430/2000 [04:31<01:38,  5.77it/s, Loss=1.0194000, Gaussian number=2102226, print grad=0.002185703255236149, Depth Loss=0.0000000]
Training progress:  72%|███████▏  | 1430/2000 [04:33<01:38,  5.77it/s, Loss=0.8825648, Gaussian number=2102226, print grad=0.002831221790984273, Depth Loss=0.0000000]
Training progress:  72%|███████▏  | 1440/2000 [04:33<01:36,  5.82it/s, Loss=0.8825648, Gaussian number=2102226, print grad=0.002831221790984273, Depth Loss=0.0000000]
Training progress:  72%|███████▏  | 1440/2000 [04:35<01:36,  5.82it/s, Loss=0.7976696, Gaussian number=2102226, print grad=0.0035290902014821768, Depth Loss=0.0000000]
Training progress:  72%|███████▎  | 1450/2000 [04:35<01:33,  5.89it/s, Loss=0.7976696, Gaussian number=2102226, print grad=0.0035290902014821768, Depth Loss=0.0000000]
Training progress:  72%|███████▎  | 1450/2000 [04:36<01:33,  5.89it/s, Loss=0.7087763, Gaussian number=2102226, print grad=0.00419954489916563, Depth Loss=0.0000000]  
Training progress:  73%|███████▎  | 1460/2000 [04:36<01:30,  5.94it/s, Loss=0.7087763, Gaussian number=2102226, print grad=0.00419954489916563, Depth Loss=0.0000000]
Training progress:  73%|███████▎  | 1460/2000 [04:39<01:30,  5.94it/s, Loss=0.9663349, Gaussian number=2102226, print grad=0.004874892067164183, Depth Loss=0.0000000]
Training progress:  74%|███████▎  | 1470/2000 [04:39<01:40,  5.27it/s, Loss=0.9663349, Gaussian number=2102226, print grad=0.004874892067164183, Depth Loss=0.0000000]
Training progress:  74%|███████▎  | 1470/2000 [04:40<01:40,  5.27it/s, Loss=0.9554520, Gaussian number=2102226, print grad=0.005627567879855633, Depth Loss=0.0000000]
Training progress:  74%|███████▍  | 1480/2000 [04:40<01:35,  5.44it/s, Loss=0.9554520, Gaussian number=2102226, print grad=0.005627567879855633, Depth Loss=0.0000000]
Training progress:  74%|███████▍  | 1480/2000 [04:42<01:35,  5.44it/s, Loss=0.9829450, Gaussian number=2102226, print grad=0.006269313860684633, Depth Loss=0.0000000]
Training progress:  74%|███████▍  | 1490/2000 [04:42<01:31,  5.60it/s, Loss=0.9829450, Gaussian number=2102226, print grad=0.006269313860684633, Depth Loss=0.0000000]
Training progress:  74%|███████▍  | 1490/2000 [04:44<01:31,  5.60it/s, Loss=0.9564768, Gaussian number=2102226, print grad=0.0069939387030899525, Depth Loss=0.0000000]
Training progress:  75%|███████▌  | 1500/2000 [04:44<01:27,  5.73it/s, Loss=0.9564768, Gaussian number=2102226, print grad=0.0069939387030899525, Depth Loss=0.0000000]
Training progress:  75%|███████▌  | 1500/2000 [04:47<01:27,  5.73it/s, Loss=1.1542804, Gaussian number=2440266, print grad=0.0006740413955412805, Depth Loss=0.0000000]
Training progress:  76%|███████▌  | 1510/2000 [04:47<01:56,  4.22it/s, Loss=1.1542804, Gaussian number=2440266, print grad=0.0006740413955412805, Depth Loss=0.0000000]
Training progress:  76%|███████▌  | 1510/2000 [04:49<01:56,  4.22it/s, Loss=1.1360161, Gaussian number=2440266, print grad=0.0013211373006924987, Depth Loss=0.0000000]
Training progress:  76%|███████▌  | 1520/2000 [04:49<01:43,  4.62it/s, Loss=1.1360161, Gaussian number=2440266, print grad=0.0013211373006924987, Depth Loss=0.0000000]
Training progress:  76%|███████▌  | 1520/2000 [04:51<01:43,  4.62it/s, Loss=0.6647089, Gaussian number=2440266, print grad=0.002073063747957349, Depth Loss=0.0000000] 
Training progress:  76%|███████▋  | 1530/2000 [04:51<01:34,  4.96it/s, Loss=0.6647089, Gaussian number=2440266, print grad=0.002073063747957349, Depth Loss=0.0000000]
Training progress:  76%|███████▋  | 1530/2000 [04:52<01:34,  4.96it/s, Loss=0.8488439, Gaussian number=2440266, print grad=0.0027421764098107815, Depth Loss=0.0000000]
Training progress:  77%|███████▋  | 1540/2000 [04:52<01:28,  5.20it/s, Loss=0.8488439, Gaussian number=2440266, print grad=0.0027421764098107815, Depth Loss=0.0000000]
Training progress:  77%|███████▋  | 1540/2000 [04:54<01:28,  5.20it/s, Loss=1.0074865, Gaussian number=2440266, print grad=0.0034106657840311527, Depth Loss=0.0000000]
Training progress:  78%|███████▊  | 1550/2000 [04:54<01:23,  5.39it/s, Loss=1.0074865, Gaussian number=2440266, print grad=0.0034106657840311527, Depth Loss=0.0000000]
Training progress:  78%|███████▊  | 1550/2000 [04:56<01:23,  5.39it/s, Loss=1.0222314, Gaussian number=2440266, print grad=0.0041444688104093075, Depth Loss=0.0000000]
Training progress:  78%|███████▊  | 1560/2000 [04:56<01:19,  5.54it/s, Loss=1.0222314, Gaussian number=2440266, print grad=0.0041444688104093075, Depth Loss=0.0000000]
Training progress:  78%|███████▊  | 1560/2000 [04:58<01:19,  5.54it/s, Loss=0.8503958, Gaussian number=2440266, print grad=0.004814721643924713, Depth Loss=0.0000000] 
Training progress:  78%|███████▊  | 1570/2000 [04:58<01:16,  5.65it/s, Loss=0.8503958, Gaussian number=2440266, print grad=0.004814721643924713, Depth Loss=0.0000000]
Training progress:  78%|███████▊  | 1570/2000 [04:59<01:16,  5.65it/s, Loss=0.6172245, Gaussian number=2440266, print grad=0.005345969460904598, Depth Loss=0.0000000]
Training progress:  79%|███████▉  | 1580/2000 [04:59<01:12,  5.78it/s, Loss=0.6172245, Gaussian number=2440266, print grad=0.005345969460904598, Depth Loss=0.0000000]
Training progress:  79%|███████▉  | 1580/2000 [05:01<01:12,  5.78it/s, Loss=0.8776618, Gaussian number=2440266, print grad=0.005976468324661255, Depth Loss=0.0000000]
Training progress:  80%|███████▉  | 1590/2000 [05:01<01:10,  5.82it/s, Loss=0.8776618, Gaussian number=2440266, print grad=0.005976468324661255, Depth Loss=0.0000000]
Training progress:  80%|███████▉  | 1590/2000 [05:03<01:10,  5.82it/s, Loss=0.7727678, Gaussian number=2440266, print grad=0.006663923617452383, Depth Loss=0.0000000]
Training progress:  80%|████████  | 1600/2000 [05:03<01:08,  5.84it/s, Loss=0.7727678, Gaussian number=2440266, print grad=0.006663923617452383, Depth Loss=0.0000000]
Training progress:  80%|████████  | 1600/2000 [05:04<01:08,  5.84it/s, Loss=1.1056224, Gaussian number=2797192, print grad=0.0005282828351482749, Depth Loss=0.0000000]
Training progress:  80%|████████  | 1610/2000 [05:04<01:07,  5.77it/s, Loss=1.1056224, Gaussian number=2797192, print grad=0.0005282828351482749, Depth Loss=0.0000000]
Training progress:  80%|████████  | 1610/2000 [05:06<01:07,  5.77it/s, Loss=1.0622981, Gaussian number=2797192, print grad=0.0012707846472039819, Depth Loss=0.0000000]
Training progress:  81%|████████  | 1620/2000 [05:06<01:05,  5.79it/s, Loss=1.0622981, Gaussian number=2797192, print grad=0.0012707846472039819, Depth Loss=0.0000000]
Training progress:  81%|████████  | 1620/2000 [05:08<01:05,  5.79it/s, Loss=0.8768711, Gaussian number=2797192, print grad=0.0019234496867284179, Depth Loss=0.0000000]
Training progress:  82%|████████▏ | 1630/2000 [05:08<01:03,  5.82it/s, Loss=0.8768711, Gaussian number=2797192, print grad=0.0019234496867284179, Depth Loss=0.0000000]
Training progress:  82%|████████▏ | 1630/2000 [05:09<01:03,  5.82it/s, Loss=0.7937902, Gaussian number=2797192, print grad=0.0024973873514682055, Depth Loss=0.0000000]
Training progress:  82%|████████▏ | 1640/2000 [05:09<01:01,  5.82it/s, Loss=0.7937902, Gaussian number=2797192, print grad=0.0024973873514682055, Depth Loss=0.0000000]
Training progress:  82%|████████▏ | 1640/2000 [05:11<01:01,  5.82it/s, Loss=0.8803383, Gaussian number=2797192, print grad=0.0030979604925960302, Depth Loss=0.0000000]
Training progress:  82%|████████▎ | 1650/2000 [05:11<01:00,  5.81it/s, Loss=0.8803383, Gaussian number=2797192, print grad=0.0030979604925960302, Depth Loss=0.0000000]
Training progress:  82%|████████▎ | 1650/2000 [05:13<01:00,  5.81it/s, Loss=0.8390889, Gaussian number=2797192, print grad=0.0036244636867195368, Depth Loss=0.0000000]
Training progress:  83%|████████▎ | 1660/2000 [05:13<00:58,  5.79it/s, Loss=0.8390889, Gaussian number=2797192, print grad=0.0036244636867195368, Depth Loss=0.0000000]
Training progress:  83%|████████▎ | 1660/2000 [05:15<00:58,  5.79it/s, Loss=0.7888418, Gaussian number=2797192, print grad=0.0041899848729372025, Depth Loss=0.0000000]
Training progress:  84%|████████▎ | 1670/2000 [05:15<00:56,  5.83it/s, Loss=0.7888418, Gaussian number=2797192, print grad=0.0041899848729372025, Depth Loss=0.0000000]
Training progress:  84%|████████▎ | 1670/2000 [05:16<00:56,  5.83it/s, Loss=0.7741488, Gaussian number=2797192, print grad=0.004700123332440853, Depth Loss=0.0000000] 
Training progress:  84%|████████▍ | 1680/2000 [05:16<00:55,  5.81it/s, Loss=0.7741488, Gaussian number=2797192, print grad=0.004700123332440853, Depth Loss=0.0000000]
Training progress:  84%|████████▍ | 1680/2000 [05:18<00:55,  5.81it/s, Loss=0.8369788, Gaussian number=2797192, print grad=0.005297516472637653, Depth Loss=0.0000000]
Training progress:  84%|████████▍ | 1690/2000 [05:18<00:53,  5.81it/s, Loss=0.8369788, Gaussian number=2797192, print grad=0.005297516472637653, Depth Loss=0.0000000]
Training progress:  84%|████████▍ | 1690/2000 [05:20<00:53,  5.81it/s, Loss=0.9297798, Gaussian number=2797192, print grad=0.005826613865792751, Depth Loss=0.0000000]
Training progress:  85%|████████▌ | 1700/2000 [05:20<00:51,  5.81it/s, Loss=0.9297798, Gaussian number=2797192, print grad=0.005826613865792751, Depth Loss=0.0000000]
Training progress:  85%|████████▌ | 1700/2000 [05:22<00:51,  5.81it/s, Loss=1.0203336, Gaussian number=3138094, print grad=0.00059452437562868, Depth Loss=0.0000000] 
Training progress:  86%|████████▌ | 1710/2000 [05:22<00:50,  5.80it/s, Loss=1.0203336, Gaussian number=3138094, print grad=0.00059452437562868, Depth Loss=0.0000000]
Training progress:  86%|████████▌ | 1710/2000 [05:23<00:50,  5.80it/s, Loss=1.0535558, Gaussian number=3138094, print grad=0.001112922909669578, Depth Loss=0.0000000]
Training progress:  86%|████████▌ | 1720/2000 [05:23<00:48,  5.82it/s, Loss=1.0535558, Gaussian number=3138094, print grad=0.001112922909669578, Depth Loss=0.0000000]
Training progress:  86%|████████▌ | 1720/2000 [05:25<00:48,  5.82it/s, Loss=1.0360386, Gaussian number=3138094, print grad=0.0016275743255391717, Depth Loss=0.0000000]
Training progress:  86%|████████▋ | 1730/2000 [05:25<00:46,  5.84it/s, Loss=1.0360386, Gaussian number=3138094, print grad=0.0016275743255391717, Depth Loss=0.0000000]
Training progress:  86%|████████▋ | 1730/2000 [05:27<00:46,  5.84it/s, Loss=1.2556181, Gaussian number=3138094, print grad=0.0021595393773168325, Depth Loss=0.0000000]
Training progress:  87%|████████▋ | 1740/2000 [05:27<00:44,  5.79it/s, Loss=1.2556181, Gaussian number=3138094, print grad=0.0021595393773168325, Depth Loss=0.0000000]
Training progress:  87%|████████▋ | 1740/2000 [05:28<00:44,  5.79it/s, Loss=1.1415884, Gaussian number=3138094, print grad=0.0026775335427373648, Depth Loss=0.0000000]
Training progress:  88%|████████▊ | 1750/2000 [05:28<00:43,  5.76it/s, Loss=1.1415884, Gaussian number=3138094, print grad=0.0026775335427373648, Depth Loss=0.0000000]
Training progress:  88%|████████▊ | 1750/2000 [05:30<00:43,  5.76it/s, Loss=0.9475401, Gaussian number=3138094, print grad=0.003219380509108305, Depth Loss=0.0000000] 
Training progress:  88%|████████▊ | 1760/2000 [05:30<00:41,  5.80it/s, Loss=0.9475401, Gaussian number=3138094, print grad=0.003219380509108305, Depth Loss=0.0000000]
Training progress:  88%|████████▊ | 1760/2000 [05:32<00:41,  5.80it/s, Loss=0.7967245, Gaussian number=3138094, print grad=0.0037046964280307293, Depth Loss=0.0000000]
Training progress:  88%|████████▊ | 1770/2000 [05:32<00:39,  5.79it/s, Loss=0.7967245, Gaussian number=3138094, print grad=0.0037046964280307293, Depth Loss=0.0000000]
Training progress:  88%|████████▊ | 1770/2000 [05:34<00:39,  5.79it/s, Loss=0.8340157, Gaussian number=3138094, print grad=0.004205510951578617, Depth Loss=0.0000000] 
Training progress:  89%|████████▉ | 1780/2000 [05:34<00:37,  5.79it/s, Loss=0.8340157, Gaussian number=3138094, print grad=0.004205510951578617, Depth Loss=0.0000000]
Training progress:  89%|████████▉ | 1780/2000 [05:35<00:37,  5.79it/s, Loss=0.8243340, Gaussian number=3138094, print grad=0.00463612237945199, Depth Loss=0.0000000] 
Training progress:  90%|████████▉ | 1790/2000 [05:35<00:36,  5.82it/s, Loss=0.8243340, Gaussian number=3138094, print grad=0.00463612237945199, Depth Loss=0.0000000]
Training progress:  90%|████████▉ | 1790/2000 [05:37<00:36,  5.82it/s, Loss=0.8249582, Gaussian number=3138094, print grad=0.005173060577362776, Depth Loss=0.0000000]
Training progress:  90%|█████████ | 1800/2000 [05:37<00:34,  5.77it/s, Loss=0.8249582, Gaussian number=3138094, print grad=0.005173060577362776, Depth Loss=0.0000000]
Training progress:  90%|█████████ | 1800/2000 [05:39<00:34,  5.77it/s, Loss=0.9954242, Gaussian number=3493233, print grad=0.00041513366159051657, Depth Loss=0.0000000]
Training progress:  90%|█████████ | 1810/2000 [05:39<00:33,  5.71it/s, Loss=0.9954242, Gaussian number=3493233, print grad=0.00041513366159051657, Depth Loss=0.0000000]
Training progress:  90%|█████████ | 1810/2000 [05:41<00:33,  5.71it/s, Loss=0.9032929, Gaussian number=3493233, print grad=0.000964321952778846, Depth Loss=0.0000000]  
Training progress:  91%|█████████ | 1820/2000 [05:41<00:31,  5.70it/s, Loss=0.9032929, Gaussian number=3493233, print grad=0.000964321952778846, Depth Loss=0.0000000]
Training progress:  91%|█████████ | 1820/2000 [05:42<00:31,  5.70it/s, Loss=0.8342426, Gaussian number=3493233, print grad=0.0013348666252568364, Depth Loss=0.0000000]
Training progress:  92%|█████████▏| 1830/2000 [05:42<00:29,  5.69it/s, Loss=0.8342426, Gaussian number=3493233, print grad=0.0013348666252568364, Depth Loss=0.0000000]
Training progress:  92%|█████████▏| 1830/2000 [05:44<00:29,  5.69it/s, Loss=0.7937709, Gaussian number=3493233, print grad=0.0018553134286776185, Depth Loss=0.0000000]
Training progress:  92%|█████████▏| 1840/2000 [05:44<00:28,  5.70it/s, Loss=0.7937709, Gaussian number=3493233, print grad=0.0018553134286776185, Depth Loss=0.0000000]
Training progress:  92%|█████████▏| 1840/2000 [05:46<00:28,  5.70it/s, Loss=0.8338932, Gaussian number=3493233, print grad=0.0023361584171652794, Depth Loss=0.0000000]
Training progress:  92%|█████████▎| 1850/2000 [05:46<00:26,  5.67it/s, Loss=0.8338932, Gaussian number=3493233, print grad=0.0023361584171652794, Depth Loss=0.0000000]
Training progress:  92%|█████████▎| 1850/2000 [05:48<00:26,  5.67it/s, Loss=0.9492416, Gaussian number=3493233, print grad=0.002790923463180661, Depth Loss=0.0000000] 
Training progress:  93%|█████████▎| 1860/2000 [05:48<00:24,  5.70it/s, Loss=0.9492416, Gaussian number=3493233, print grad=0.002790923463180661, Depth Loss=0.0000000]
Training progress:  93%|█████████▎| 1860/2000 [05:49<00:24,  5.70it/s, Loss=1.0440527, Gaussian number=3493233, print grad=0.003327320795506239, Depth Loss=0.0000000]
Training progress:  94%|█████████▎| 1870/2000 [05:49<00:22,  5.67it/s, Loss=1.0440527, Gaussian number=3493233, print grad=0.003327320795506239, Depth Loss=0.0000000]
Training progress:  94%|█████████▎| 1870/2000 [05:51<00:22,  5.67it/s, Loss=0.7592839, Gaussian number=3493233, print grad=0.003856988623738289, Depth Loss=0.0000000]
Training progress:  94%|█████████▍| 1880/2000 [05:51<00:20,  5.72it/s, Loss=0.7592839, Gaussian number=3493233, print grad=0.003856988623738289, Depth Loss=0.0000000]
Training progress:  94%|█████████▍| 1880/2000 [05:53<00:20,  5.72it/s, Loss=0.9530778, Gaussian number=3493233, print grad=0.004367721267044544, Depth Loss=0.0000000]
Training progress:  94%|█████████▍| 1890/2000 [05:53<00:19,  5.72it/s, Loss=0.9530778, Gaussian number=3493233, print grad=0.004367721267044544, Depth Loss=0.0000000]
Training progress:  94%|█████████▍| 1890/2000 [05:55<00:19,  5.72it/s, Loss=1.0099785, Gaussian number=3493233, print grad=0.004815669264644384, Depth Loss=0.0000000]
Training progress:  95%|█████████▌| 1900/2000 [05:55<00:17,  5.70it/s, Loss=1.0099785, Gaussian number=3493233, print grad=0.004815669264644384, Depth Loss=0.0000000]
Training progress:  95%|█████████▌| 1900/2000 [05:57<00:17,  5.70it/s, Loss=0.9137136, Gaussian number=3848563, print grad=0.0003851393994409591, Depth Loss=0.0000000]
Training progress:  96%|█████████▌| 1910/2000 [05:57<00:15,  5.64it/s, Loss=0.9137136, Gaussian number=3848563, print grad=0.0003851393994409591, Depth Loss=0.0000000]
Training progress:  96%|█████████▌| 1910/2000 [05:58<00:15,  5.64it/s, Loss=0.9570601, Gaussian number=3848563, print grad=0.0008528432808816433, Depth Loss=0.0000000]
Training progress:  96%|█████████▌| 1920/2000 [05:58<00:14,  5.63it/s, Loss=0.9570601, Gaussian number=3848563, print grad=0.0008528432808816433, Depth Loss=0.0000000]
Training progress:  96%|█████████▌| 1920/2000 [06:00<00:14,  5.63it/s, Loss=0.7311460, Gaussian number=3848563, print grad=0.0013195512583479285, Depth Loss=0.0000000]
Training progress:  96%|█████████▋| 1930/2000 [06:00<00:12,  5.60it/s, Loss=0.7311460, Gaussian number=3848563, print grad=0.0013195512583479285, Depth Loss=0.0000000]
Training progress:  96%|█████████▋| 1930/2000 [06:02<00:12,  5.60it/s, Loss=0.9523158, Gaussian number=3848563, print grad=0.0017827926203608513, Depth Loss=0.0000000]
Training progress:  97%|█████████▋| 1940/2000 [06:02<00:10,  5.60it/s, Loss=0.9523158, Gaussian number=3848563, print grad=0.0017827926203608513, Depth Loss=0.0000000]
Training progress:  97%|█████████▋| 1940/2000 [06:04<00:10,  5.60it/s, Loss=0.8139233, Gaussian number=3848563, print grad=0.002238127402961254, Depth Loss=0.0000000] 
Training progress:  98%|█████████▊| 1950/2000 [06:04<00:08,  5.60it/s, Loss=0.8139233, Gaussian number=3848563, print grad=0.002238127402961254, Depth Loss=0.0000000]
Training progress:  98%|█████████▊| 1950/2000 [06:05<00:08,  5.60it/s, Loss=1.1218025, Gaussian number=3848563, print grad=0.0026505389250814915, Depth Loss=0.0000000]
Training progress:  98%|█████████▊| 1960/2000 [06:05<00:07,  5.62it/s, Loss=1.1218025, Gaussian number=3848563, print grad=0.0026505389250814915, Depth Loss=0.0000000]
Training progress:  98%|█████████▊| 1960/2000 [06:07<00:07,  5.62it/s, Loss=0.9814209, Gaussian number=3848563, print grad=0.0030704482924193144, Depth Loss=0.0000000]
Training progress:  98%|█████████▊| 1970/2000 [06:07<00:05,  5.57it/s, Loss=0.9814209, Gaussian number=3848563, print grad=0.0030704482924193144, Depth Loss=0.0000000]
Training progress:  98%|█████████▊| 1970/2000 [06:09<00:05,  5.57it/s, Loss=0.9255861, Gaussian number=3848563, print grad=0.0035662297159433365, Depth Loss=0.0000000]
Training progress:  99%|█████████▉| 1980/2000 [06:09<00:03,  5.56it/s, Loss=0.9255861, Gaussian number=3848563, print grad=0.0035662297159433365, Depth Loss=0.0000000]
Training progress:  99%|█████████▉| 1980/2000 [06:11<00:03,  5.56it/s, Loss=0.8639198, Gaussian number=3848563, print grad=0.0039942278526723385, Depth Loss=0.0000000]
Training progress: 100%|█████████▉| 1990/2000 [06:11<00:01,  5.54it/s, Loss=0.8639198, Gaussian number=3848563, print grad=0.0039942278526723385, Depth Loss=0.0000000]
Training progress: 100%|█████████▉| 1990/2000 [06:13<00:01,  5.54it/s, Loss=0.6582114, Gaussian number=3848563, print grad=0.004524324554949999, Depth Loss=0.0000000] 
Training progress: 100%|██████████| 2000/2000 [06:13<00:00,  5.53it/s, Loss=0.6582114, Gaussian number=3848563, print grad=0.004524324554949999, Depth Loss=0.0000000]
Training progress: 100%|██████████| 2000/2000 [06:13<00:00,  5.36it/s, Loss=0.6582114, Gaussian number=3848563, print grad=0.004524324554949999, Depth Loss=0.0000000]
Iteration 100 [14/11 19:08:43]

[ITER 100] Evaluating test: WD 1.319426, PSNR 12.8690 [14/11 19:08:45]

[ITER 100] Evaluating train: WD 1.346346, PSNR 13.2665 [14/11 19:08:45]
Gaussian number:182686,print gradients:0.00015954971604514867 [14/11 19:08:45]
Iteration 200 [14/11 19:09:02]

[ITER 200] Evaluating test: WD 1.187943, PSNR 14.1785 [14/11 19:09:04]

[ITER 200] Evaluating train: WD 1.191507, PSNR 14.6090 [14/11 19:09:05]
Gaussian number:182686,print gradients:0.0002141549193765968 [14/11 19:09:05]
Iteration 300 [14/11 19:09:21]

[ITER 300] Evaluating test: WD 1.095861, PSNR 14.9752 [14/11 19:09:23]

[ITER 300] Evaluating train: WD 1.097745, PSNR 15.5115 [14/11 19:09:24]
Gaussian number:182686,print gradients:0.0002529206103645265 [14/11 19:09:24]
Iteration 400 [14/11 19:09:40]

[ITER 400] Evaluating test: WD 1.049665, PSNR 15.4915 [14/11 19:09:42]

[ITER 400] Evaluating train: WD 1.044332, PSNR 16.2021 [14/11 19:09:42]
Gaussian number:182686,print gradients:0.00028067739913240075 [14/11 19:09:42]
Iteration 500 [14/11 19:09:59]

[ITER 500] Evaluating test: WD 0.990447, PSNR 15.9560 [14/11 19:10:01]

[ITER 500] Evaluating train: WD 1.037395, PSNR 16.2132 [14/11 19:10:01]
Gaussian number:182686,print gradients:0.00030920925200916827 [14/11 19:10:01]
Iteration 600 [14/11 19:10:17]

[ITER 600] Evaluating test: WD 0.956450, PSNR 16.2453 [14/11 19:10:19]

[ITER 600] Evaluating train: WD 0.978475, PSNR 16.5303 [14/11 19:10:19]
Gaussian number:182686,print gradients:0.00033075929968617857 [14/11 19:10:20]
Iteration 700 [14/11 19:10:37]

[ITER 700] Evaluating test: WD 0.949643, PSNR 16.4225 [14/11 19:10:39]

[ITER 700] Evaluating train: WD 0.972672, PSNR 16.6585 [14/11 19:10:39]
Gaussian number:270280,print gradients:0.0003941896720789373 [14/11 19:10:39]
Iteration 800 [14/11 19:10:55]

[ITER 800] Evaluating test: WD 0.905419, PSNR 16.5737 [14/11 19:10:57]

[ITER 800] Evaluating train: WD 0.936774, PSNR 16.7682 [14/11 19:10:58]
Gaussian number:400305,print gradients:0.0003059640002902597 [14/11 19:10:58]
Iteration 900 [14/11 19:11:14]

[ITER 900] Evaluating test: WD 0.900619, PSNR 16.6574 [14/11 19:11:16]

[ITER 900] Evaluating train: WD 0.962910, PSNR 16.9923 [14/11 19:11:17]
Gaussian number:554666,print gradients:nan [14/11 19:11:17]
Iteration 1000 [14/11 19:11:33]

[ITER 1000] Evaluating test: WD 0.884955, PSNR 16.7085 [14/11 19:11:35]

[ITER 1000] Evaluating train: WD 0.942935, PSNR 16.7197 [14/11 19:11:36]
Gaussian number:740903,print gradients:nan [14/11 19:11:36]
Iteration 1100 [14/11 19:11:53]
Iteration 1200 [14/11 19:12:10]
Iteration 1300 [14/11 19:12:28]
Iteration 1400 [14/11 19:12:47]
Iteration 1500 [14/11 19:13:05]

[ITER 1500] Evaluating test: WD 0.869652, PSNR 16.5719 [14/11 19:13:07]

[ITER 1500] Evaluating train: WD 0.939155, PSNR 16.9985 [14/11 19:13:07]
Gaussian number:2102226,print gradients:nan [14/11 19:13:07]
Iteration 1600 [14/11 19:13:24]
Iteration 1700 [14/11 19:13:41]
Iteration 1800 [14/11 19:13:58]
Iteration 1900 [14/11 19:14:16]
Iteration 2000 [14/11 19:14:34]

[ITER 2000] Evaluating test: WD 0.903300, PSNR 16.2335 [14/11 19:14:36]

[ITER 2000] Evaluating train: WD 1.023945, PSNR 16.5227 [14/11 19:14:36]
Gaussian number:3848563,print gradients:nan [14/11 19:14:36]

[ITER 2000] Saving Gaussians [14/11 19:14:36]

Training complete. [14/11 19:15:14]
