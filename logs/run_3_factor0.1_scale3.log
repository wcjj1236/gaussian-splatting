Optimizing /home/cw4287/gaussian-model/train1
Output folder: /home/cw4287/gaussian-model/train1 [14/11 19:15:20]
Tensorboard not available: not logging progress [14/11 19:15:20]
------------LLFF HOLD------------- [14/11 19:15:22]

Reading camera 1/301
Reading camera 2/301
Reading camera 3/301
Reading camera 4/301
Reading camera 5/301
Reading camera 6/301
Reading camera 7/301
Reading camera 8/301
Reading camera 9/301
Reading camera 10/301
Reading camera 11/301
Reading camera 12/301
Reading camera 13/301
Reading camera 14/301
Reading camera 15/301
Reading camera 16/301
Reading camera 17/301
Reading camera 18/301
Reading camera 19/301
Reading camera 20/301
Reading camera 21/301
Reading camera 22/301
Reading camera 23/301
Reading camera 24/301
Reading camera 25/301
Reading camera 26/301
Reading camera 27/301
Reading camera 28/301
Reading camera 29/301
Reading camera 30/301
Reading camera 31/301
Reading camera 32/301
Reading camera 33/301
Reading camera 34/301
Reading camera 35/301
Reading camera 36/301
Reading camera 37/301
Reading camera 38/301
Reading camera 39/301
Reading camera 40/301
Reading camera 41/301
Reading camera 42/301
Reading camera 43/301
Reading camera 44/301
Reading camera 45/301
Reading camera 46/301
Reading camera 47/301
Reading camera 48/301
Reading camera 49/301
Reading camera 50/301
Reading camera 51/301
Reading camera 52/301
Reading camera 53/301
Reading camera 54/301
Reading camera 55/301
Reading camera 56/301
Reading camera 57/301
Reading camera 58/301
Reading camera 59/301
Reading camera 60/301
Reading camera 61/301
Reading camera 62/301
Reading camera 63/301
Reading camera 64/301
Reading camera 65/301
Reading camera 66/301
Reading camera 67/301
Reading camera 68/301
Reading camera 69/301
Reading camera 70/301
Reading camera 71/301
Reading camera 72/301
Reading camera 73/301
Reading camera 74/301
Reading camera 75/301
Reading camera 76/301
Reading camera 77/301
Reading camera 78/301
Reading camera 79/301
Reading camera 80/301
Reading camera 81/301
Reading camera 82/301
Reading camera 83/301
Reading camera 84/301
Reading camera 85/301
Reading camera 86/301
Reading camera 87/301
Reading camera 88/301
Reading camera 89/301
Reading camera 90/301
Reading camera 91/301
Reading camera 92/301
Reading camera 93/301
Reading camera 94/301
Reading camera 95/301
Reading camera 96/301
Reading camera 97/301
Reading camera 98/301
Reading camera 99/301
Reading camera 100/301
Reading camera 101/301
Reading camera 102/301
Reading camera 103/301
Reading camera 104/301
Reading camera 105/301
Reading camera 106/301
Reading camera 107/301
Reading camera 108/301
Reading camera 109/301
Reading camera 110/301
Reading camera 111/301
Reading camera 112/301
Reading camera 113/301
Reading camera 114/301
Reading camera 115/301
Reading camera 116/301
Reading camera 117/301
Reading camera 118/301
Reading camera 119/301
Reading camera 120/301
Reading camera 121/301
Reading camera 122/301
Reading camera 123/301
Reading camera 124/301
Reading camera 125/301
Reading camera 126/301
Reading camera 127/301
Reading camera 128/301
Reading camera 129/301
Reading camera 130/301
Reading camera 131/301
Reading camera 132/301
Reading camera 133/301
Reading camera 134/301
Reading camera 135/301
Reading camera 136/301
Reading camera 137/301
Reading camera 138/301
Reading camera 139/301
Reading camera 140/301
Reading camera 141/301
Reading camera 142/301
Reading camera 143/301
Reading camera 144/301
Reading camera 145/301
Reading camera 146/301
Reading camera 147/301
Reading camera 148/301
Reading camera 149/301
Reading camera 150/301
Reading camera 151/301
Reading camera 152/301
Reading camera 153/301
Reading camera 154/301
Reading camera 155/301
Reading camera 156/301
Reading camera 157/301
Reading camera 158/301
Reading camera 159/301
Reading camera 160/301
Reading camera 161/301
Reading camera 162/301
Reading camera 163/301
Reading camera 164/301
Reading camera 165/301
Reading camera 166/301
Reading camera 167/301
Reading camera 168/301
Reading camera 169/301
Reading camera 170/301
Reading camera 171/301
Reading camera 172/301
Reading camera 173/301
Reading camera 174/301
Reading camera 175/301
Reading camera 176/301
Reading camera 177/301
Reading camera 178/301
Reading camera 179/301
Reading camera 180/301
Reading camera 181/301
Reading camera 182/301
Reading camera 183/301
Reading camera 184/301
Reading camera 185/301
Reading camera 186/301
Reading camera 187/301
Reading camera 188/301
Reading camera 189/301
Reading camera 190/301
Reading camera 191/301
Reading camera 192/301
Reading camera 193/301
Reading camera 194/301
Reading camera 195/301
Reading camera 196/301
Reading camera 197/301
Reading camera 198/301
Reading camera 199/301
Reading camera 200/301
Reading camera 201/301
Reading camera 202/301
Reading camera 203/301
Reading camera 204/301
Reading camera 205/301
Reading camera 206/301
Reading camera 207/301
Reading camera 208/301
Reading camera 209/301
Reading camera 210/301
Reading camera 211/301
Reading camera 212/301
Reading camera 213/301
Reading camera 214/301
Reading camera 215/301
Reading camera 216/301
Reading camera 217/301
Reading camera 218/301
Reading camera 219/301
Reading camera 220/301
Reading camera 221/301
Reading camera 222/301
Reading camera 223/301
Reading camera 224/301
Reading camera 225/301
Reading camera 226/301
Reading camera 227/301
Reading camera 228/301
Reading camera 229/301
Reading camera 230/301
Reading camera 231/301
Reading camera 232/301
Reading camera 233/301
Reading camera 234/301
Reading camera 235/301
Reading camera 236/301
Reading camera 237/301
Reading camera 238/301
Reading camera 239/301
Reading camera 240/301
Reading camera 241/301
Reading camera 242/301
Reading camera 243/301
Reading camera 244/301
Reading camera 245/301
Reading camera 246/301
Reading camera 247/301
Reading camera 248/301
Reading camera 249/301
Reading camera 250/301
Reading camera 251/301
Reading camera 252/301
Reading camera 253/301
Reading camera 254/301
Reading camera 255/301
Reading camera 256/301
Reading camera 257/301
Reading camera 258/301
Reading camera 259/301
Reading camera 260/301
Reading camera 261/301
Reading camera 262/301
Reading camera 263/301
Reading camera 264/301
Reading camera 265/301
Reading camera 266/301
Reading camera 267/301
Reading camera 268/301
Reading camera 269/301
Reading camera 270/301
Reading camera 271/301
Reading camera 272/301
Reading camera 273/301
Reading camera 274/301
Reading camera 275/301
Reading camera 276/301
Reading camera 277/301
Reading camera 278/301
Reading camera 279/301
Reading camera 280/301
Reading camera 281/301
Reading camera 282/301
Reading camera 283/301
Reading camera 284/301
Reading camera 285/301
Reading camera 286/301
Reading camera 287/301
Reading camera 288/301
Reading camera 289/301
Reading camera 290/301
Reading camera 291/301
Reading camera 292/301
Reading camera 293/301
Reading camera 294/301
Reading camera 295/301
Reading camera 296/301
Reading camera 297/301
Reading camera 298/301
Reading camera 299/301
Reading camera 300/301
Reading camera 301/301 [14/11 19:15:22]
Loading Training Cameras [14/11 19:15:22]
Loading Test Cameras [14/11 19:15:53]
Number of points at initialisation :  182686 [14/11 19:15:58]

Training progress:   0%|          | 0/2000 [00:00<?, ?it/s]/ext3/conda/envs/3dgs/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/ext3/conda/envs/3dgs/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
/ext3/conda/envs/3dgs/lib/python3.12/site-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at /opt/conda/conda-bld/pytorch_1712609048481/work/aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return F.conv2d(input, weight, bias, self.stride,

Training progress:   0%|          | 0/2000 [00:05<?, ?it/s, Loss=2.3783772, Gaussian number=182686, print grad=0.0011359151685610414, Depth Loss=0.0000000]
Training progress:   0%|          | 10/2000 [00:05<19:00,  1.75it/s, Loss=2.3783772, Gaussian number=182686, print grad=0.0011359151685610414, Depth Loss=0.0000000]
Training progress:   0%|          | 10/2000 [00:07<19:00,  1.75it/s, Loss=2.2314488, Gaussian number=182686, print grad=0.002938481280580163, Depth Loss=0.0000000] 
Training progress:   1%|          | 20/2000 [00:07<12:06,  2.73it/s, Loss=2.2314488, Gaussian number=182686, print grad=0.002938481280580163, Depth Loss=0.0000000]
Training progress:   1%|          | 20/2000 [00:10<12:06,  2.73it/s, Loss=2.2172611, Gaussian number=182686, print grad=0.00462425546720624, Depth Loss=0.0000000] 
Training progress:   2%|▏         | 30/2000 [00:10<09:53,  3.32it/s, Loss=2.2172611, Gaussian number=182686, print grad=0.00462425546720624, Depth Loss=0.0000000]
Training progress:   2%|▏         | 30/2000 [00:12<09:53,  3.32it/s, Loss=2.3948207, Gaussian number=182686, print grad=0.006383905652910471, Depth Loss=0.0000000]
Training progress:   2%|▏         | 40/2000 [00:12<08:50,  3.70it/s, Loss=2.3948207, Gaussian number=182686, print grad=0.006383905652910471, Depth Loss=0.0000000]
Training progress:   2%|▏         | 40/2000 [00:14<08:50,  3.70it/s, Loss=1.8333723, Gaussian number=182686, print grad=0.007810349576175213, Depth Loss=0.0000000]
Training progress:   2%|▎         | 50/2000 [00:14<08:13,  3.95it/s, Loss=1.8333723, Gaussian number=182686, print grad=0.007810349576175213, Depth Loss=0.0000000]
Training progress:   2%|▎         | 50/2000 [00:16<08:13,  3.95it/s, Loss=2.0390277, Gaussian number=182686, print grad=0.009884071536362171, Depth Loss=0.0000000]
Training progress:   3%|▎         | 60/2000 [00:16<07:50,  4.13it/s, Loss=2.0390277, Gaussian number=182686, print grad=0.009884071536362171, Depth Loss=0.0000000]
Training progress:   3%|▎         | 60/2000 [00:19<07:50,  4.13it/s, Loss=1.8286071, Gaussian number=182686, print grad=0.012498490512371063, Depth Loss=0.0000000]
Training progress:   4%|▎         | 70/2000 [00:19<07:35,  4.24it/s, Loss=1.8286071, Gaussian number=182686, print grad=0.012498490512371063, Depth Loss=0.0000000]
Training progress:   4%|▎         | 70/2000 [00:21<07:35,  4.24it/s, Loss=2.2346750, Gaussian number=182686, print grad=0.014555306173861027, Depth Loss=0.0000000]
Training progress:   4%|▍         | 80/2000 [00:21<07:23,  4.33it/s, Loss=2.2346750, Gaussian number=182686, print grad=0.014555306173861027, Depth Loss=0.0000000]
Training progress:   4%|▍         | 80/2000 [00:23<07:23,  4.33it/s, Loss=1.8856142, Gaussian number=182686, print grad=0.016671601682901382, Depth Loss=0.0000000]
Training progress:   4%|▍         | 90/2000 [00:23<07:12,  4.42it/s, Loss=1.8856142, Gaussian number=182686, print grad=0.016671601682901382, Depth Loss=0.0000000]
Training progress:   4%|▍         | 90/2000 [00:25<07:12,  4.42it/s, Loss=1.8458002, Gaussian number=182686, print grad=0.01918627694249153, Depth Loss=0.0000000] 
Training progress:   5%|▌         | 100/2000 [00:25<07:24,  4.28it/s, Loss=1.8458002, Gaussian number=182686, print grad=0.01918627694249153, Depth Loss=0.0000000]
Training progress:   5%|▌         | 100/2000 [00:31<07:24,  4.28it/s, Loss=2.1331672, Gaussian number=182686, print grad=0.021918661892414093, Depth Loss=0.0000000]
Training progress:   6%|▌         | 110/2000 [00:31<10:06,  3.11it/s, Loss=2.1331672, Gaussian number=182686, print grad=0.021918661892414093, Depth Loss=0.0000000]
Training progress:   6%|▌         | 110/2000 [00:33<10:06,  3.11it/s, Loss=1.7841235, Gaussian number=182686, print grad=0.024497484788298607, Depth Loss=0.0000000]
Training progress:   6%|▌         | 120/2000 [00:33<09:01,  3.47it/s, Loss=1.7841235, Gaussian number=182686, print grad=0.024497484788298607, Depth Loss=0.0000000]
Training progress:   6%|▌         | 120/2000 [00:35<09:01,  3.47it/s, Loss=2.0410364, Gaussian number=182686, print grad=0.02774042636156082, Depth Loss=0.0000000] 
Training progress:   6%|▋         | 130/2000 [00:35<08:16,  3.76it/s, Loss=2.0410364, Gaussian number=182686, print grad=0.02774042636156082, Depth Loss=0.0000000]
Training progress:   6%|▋         | 130/2000 [00:37<08:16,  3.76it/s, Loss=1.8672863, Gaussian number=182686, print grad=0.03097989223897457, Depth Loss=0.0000000]
Training progress:   7%|▋         | 140/2000 [00:37<07:45,  4.00it/s, Loss=1.8672863, Gaussian number=182686, print grad=0.03097989223897457, Depth Loss=0.0000000]
Training progress:   7%|▋         | 140/2000 [00:39<07:45,  4.00it/s, Loss=1.6533956, Gaussian number=182686, print grad=0.033903006464242935, Depth Loss=0.0000000]
Training progress:   8%|▊         | 150/2000 [00:39<07:21,  4.19it/s, Loss=1.6533956, Gaussian number=182686, print grad=0.033903006464242935, Depth Loss=0.0000000]
Training progress:   8%|▊         | 150/2000 [00:41<07:21,  4.19it/s, Loss=1.7400509, Gaussian number=182686, print grad=0.03757986053824425, Depth Loss=0.0000000] 
Training progress:   8%|▊         | 160/2000 [00:41<07:04,  4.34it/s, Loss=1.7400509, Gaussian number=182686, print grad=0.03757986053824425, Depth Loss=0.0000000]
Training progress:   8%|▊         | 160/2000 [00:43<07:04,  4.34it/s, Loss=1.7271898, Gaussian number=182686, print grad=0.04048488661646843, Depth Loss=0.0000000]
Training progress:   8%|▊         | 170/2000 [00:43<06:52,  4.44it/s, Loss=1.7271898, Gaussian number=182686, print grad=0.04048488661646843, Depth Loss=0.0000000]
Training progress:   8%|▊         | 170/2000 [00:46<06:52,  4.44it/s, Loss=1.4829353, Gaussian number=182686, print grad=0.04382438212633133, Depth Loss=0.0000000]
Training progress:   9%|▉         | 180/2000 [00:46<06:43,  4.51it/s, Loss=1.4829353, Gaussian number=182686, print grad=0.04382438212633133, Depth Loss=0.0000000]
Training progress:   9%|▉         | 180/2000 [00:48<06:43,  4.51it/s, Loss=1.9107337, Gaussian number=182686, print grad=0.046718779951334, Depth Loss=0.0000000]  
Training progress:  10%|▉         | 190/2000 [00:48<06:36,  4.57it/s, Loss=1.9107337, Gaussian number=182686, print grad=0.046718779951334, Depth Loss=0.0000000]
Training progress:  10%|▉         | 190/2000 [00:50<06:36,  4.57it/s, Loss=1.6586108, Gaussian number=182686, print grad=0.05031146854162216, Depth Loss=0.0000000]
Training progress:  10%|█         | 200/2000 [00:50<06:31,  4.60it/s, Loss=1.6586108, Gaussian number=182686, print grad=0.05031146854162216, Depth Loss=0.0000000]
Training progress:  10%|█         | 200/2000 [00:55<06:31,  4.60it/s, Loss=1.8768283, Gaussian number=182686, print grad=0.05394512042403221, Depth Loss=0.0000000]
Training progress:  10%|█         | 210/2000 [00:55<09:09,  3.26it/s, Loss=1.8768283, Gaussian number=182686, print grad=0.05394512042403221, Depth Loss=0.0000000]
Training progress:  10%|█         | 210/2000 [00:57<09:09,  3.26it/s, Loss=1.5281344, Gaussian number=182686, print grad=0.05723368749022484, Depth Loss=0.0000000]
Training progress:  11%|█         | 220/2000 [00:57<08:15,  3.59it/s, Loss=1.5281344, Gaussian number=182686, print grad=0.05723368749022484, Depth Loss=0.0000000]
Training progress:  11%|█         | 220/2000 [00:59<08:15,  3.59it/s, Loss=1.7126364, Gaussian number=182686, print grad=0.06066315993666649, Depth Loss=0.0000000]
Training progress:  12%|█▏        | 230/2000 [00:59<07:37,  3.87it/s, Loss=1.7126364, Gaussian number=182686, print grad=0.06066315993666649, Depth Loss=0.0000000]
Training progress:  12%|█▏        | 230/2000 [01:01<07:37,  3.87it/s, Loss=2.0962861, Gaussian number=182686, print grad=0.06390583515167236, Depth Loss=0.0000000]
Training progress:  12%|█▏        | 240/2000 [01:01<07:09,  4.10it/s, Loss=2.0962861, Gaussian number=182686, print grad=0.06390583515167236, Depth Loss=0.0000000]
Training progress:  12%|█▏        | 240/2000 [01:03<07:09,  4.10it/s, Loss=1.6181603, Gaussian number=182686, print grad=0.06748407334089279, Depth Loss=0.0000000]
Training progress:  12%|█▎        | 250/2000 [01:03<06:48,  4.28it/s, Loss=1.6181603, Gaussian number=182686, print grad=0.06748407334089279, Depth Loss=0.0000000]
Training progress:  12%|█▎        | 250/2000 [01:06<06:48,  4.28it/s, Loss=1.8224185, Gaussian number=182686, print grad=0.07084206491708755, Depth Loss=0.0000000]
Training progress:  13%|█▎        | 260/2000 [01:06<06:34,  4.41it/s, Loss=1.8224185, Gaussian number=182686, print grad=0.07084206491708755, Depth Loss=0.0000000]
Training progress:  13%|█▎        | 260/2000 [01:08<06:34,  4.41it/s, Loss=1.2826644, Gaussian number=182686, print grad=0.07438243180513382, Depth Loss=0.0000000]
Training progress:  14%|█▎        | 270/2000 [01:08<06:23,  4.51it/s, Loss=1.2826644, Gaussian number=182686, print grad=0.07438243180513382, Depth Loss=0.0000000]
Training progress:  14%|█▎        | 270/2000 [01:10<06:23,  4.51it/s, Loss=1.6416481, Gaussian number=182686, print grad=0.0784185454249382, Depth Loss=0.0000000] 
Training progress:  14%|█▍        | 280/2000 [01:10<06:14,  4.59it/s, Loss=1.6416481, Gaussian number=182686, print grad=0.0784185454249382, Depth Loss=0.0000000]
Training progress:  14%|█▍        | 280/2000 [01:12<06:14,  4.59it/s, Loss=1.6782522, Gaussian number=182686, print grad=0.08221787214279175, Depth Loss=0.0000000]
Training progress:  14%|█▍        | 290/2000 [01:12<06:08,  4.64it/s, Loss=1.6782522, Gaussian number=182686, print grad=0.08221787214279175, Depth Loss=0.0000000]
Training progress:  14%|█▍        | 290/2000 [01:14<06:08,  4.64it/s, Loss=1.5653432, Gaussian number=182686, print grad=0.08612806349992752, Depth Loss=0.0000000]
Training progress:  15%|█▌        | 300/2000 [01:14<06:03,  4.67it/s, Loss=1.5653432, Gaussian number=182686, print grad=0.08612806349992752, Depth Loss=0.0000000]
Training progress:  15%|█▌        | 300/2000 [01:19<06:03,  4.67it/s, Loss=1.3038736, Gaussian number=182686, print grad=0.09019117802381516, Depth Loss=0.0000000]
Training progress:  16%|█▌        | 310/2000 [01:19<08:33,  3.29it/s, Loss=1.3038736, Gaussian number=182686, print grad=0.09019117802381516, Depth Loss=0.0000000]
Training progress:  16%|█▌        | 310/2000 [01:21<08:33,  3.29it/s, Loss=1.3595805, Gaussian number=182686, print grad=0.09310151636600494, Depth Loss=0.0000000]
Training progress:  16%|█▌        | 320/2000 [01:21<07:42,  3.63it/s, Loss=1.3595805, Gaussian number=182686, print grad=0.09310151636600494, Depth Loss=0.0000000]
Training progress:  16%|█▌        | 320/2000 [01:23<07:42,  3.63it/s, Loss=1.7933655, Gaussian number=182686, print grad=0.0966099351644516, Depth Loss=0.0000000] 
Training progress:  16%|█▋        | 330/2000 [01:23<07:06,  3.92it/s, Loss=1.7933655, Gaussian number=182686, print grad=0.0966099351644516, Depth Loss=0.0000000]
Training progress:  16%|█▋        | 330/2000 [01:25<07:06,  3.92it/s, Loss=1.3026082, Gaussian number=182686, print grad=0.10060949623584747, Depth Loss=0.0000000]
Training progress:  17%|█▋        | 340/2000 [01:25<06:41,  4.13it/s, Loss=1.3026082, Gaussian number=182686, print grad=0.10060949623584747, Depth Loss=0.0000000]
Training progress:  17%|█▋        | 340/2000 [01:27<06:41,  4.13it/s, Loss=1.3823370, Gaussian number=182686, print grad=0.10436199605464935, Depth Loss=0.0000000]
Training progress:  18%|█▊        | 350/2000 [01:27<06:23,  4.30it/s, Loss=1.3823370, Gaussian number=182686, print grad=0.10436199605464935, Depth Loss=0.0000000]
Training progress:  18%|█▊        | 350/2000 [01:30<06:23,  4.30it/s, Loss=1.3254269, Gaussian number=182686, print grad=0.10883773118257523, Depth Loss=0.0000000]
Training progress:  18%|█▊        | 360/2000 [01:30<06:09,  4.44it/s, Loss=1.3254269, Gaussian number=182686, print grad=0.10883773118257523, Depth Loss=0.0000000]
Training progress:  18%|█▊        | 360/2000 [01:32<06:09,  4.44it/s, Loss=1.2905317, Gaussian number=182686, print grad=0.11278603971004486, Depth Loss=0.0000000]
Training progress:  18%|█▊        | 370/2000 [01:32<05:59,  4.54it/s, Loss=1.2905317, Gaussian number=182686, print grad=0.11278603971004486, Depth Loss=0.0000000]
Training progress:  18%|█▊        | 370/2000 [01:34<05:59,  4.54it/s, Loss=1.7296149, Gaussian number=182686, print grad=0.11611422896385193, Depth Loss=0.0000000]
Training progress:  19%|█▉        | 380/2000 [01:34<05:51,  4.61it/s, Loss=1.7296149, Gaussian number=182686, print grad=0.11611422896385193, Depth Loss=0.0000000]
Training progress:  19%|█▉        | 380/2000 [01:36<05:51,  4.61it/s, Loss=1.5320351, Gaussian number=182686, print grad=0.12019754946231842, Depth Loss=0.0000000]
Training progress:  20%|█▉        | 390/2000 [01:36<05:45,  4.66it/s, Loss=1.5320351, Gaussian number=182686, print grad=0.12019754946231842, Depth Loss=0.0000000]
Training progress:  20%|█▉        | 390/2000 [01:38<05:45,  4.66it/s, Loss=1.8360463, Gaussian number=182686, print grad=0.1240188479423523, Depth Loss=0.0000000] 
Training progress:  20%|██        | 400/2000 [01:38<05:40,  4.70it/s, Loss=1.8360463, Gaussian number=182686, print grad=0.1240188479423523, Depth Loss=0.0000000]
Training progress:  20%|██        | 400/2000 [01:43<05:40,  4.70it/s, Loss=1.5708453, Gaussian number=182686, print grad=0.12861105799674988, Depth Loss=0.0000000]
Training progress:  20%|██        | 410/2000 [01:43<08:00,  3.31it/s, Loss=1.5708453, Gaussian number=182686, print grad=0.12861105799674988, Depth Loss=0.0000000]
Training progress:  20%|██        | 410/2000 [01:45<08:00,  3.31it/s, Loss=1.4015624, Gaussian number=182686, print grad=0.13289280235767365, Depth Loss=0.0000000]
Training progress:  21%|██        | 420/2000 [01:45<07:14,  3.64it/s, Loss=1.4015624, Gaussian number=182686, print grad=0.13289280235767365, Depth Loss=0.0000000]
Training progress:  21%|██        | 420/2000 [01:47<07:14,  3.64it/s, Loss=1.7617302, Gaussian number=182686, print grad=0.13731630146503448, Depth Loss=0.0000000]
Training progress:  22%|██▏       | 430/2000 [01:47<06:40,  3.92it/s, Loss=1.7617302, Gaussian number=182686, print grad=0.13731630146503448, Depth Loss=0.0000000]
Training progress:  22%|██▏       | 430/2000 [01:49<06:40,  3.92it/s, Loss=1.3658662, Gaussian number=182686, print grad=0.1414317488670349, Depth Loss=0.0000000] 
Training progress:  22%|██▏       | 440/2000 [01:49<06:17,  4.14it/s, Loss=1.3658662, Gaussian number=182686, print grad=0.1414317488670349, Depth Loss=0.0000000]
Training progress:  22%|██▏       | 440/2000 [01:51<06:17,  4.14it/s, Loss=1.5720171, Gaussian number=182686, print grad=0.1457071602344513, Depth Loss=0.0000000]
Training progress:  22%|██▎       | 450/2000 [01:51<05:59,  4.31it/s, Loss=1.5720171, Gaussian number=182686, print grad=0.1457071602344513, Depth Loss=0.0000000]
Training progress:  22%|██▎       | 450/2000 [01:53<05:59,  4.31it/s, Loss=1.6256691, Gaussian number=182686, print grad=0.14985758066177368, Depth Loss=0.0000000]
Training progress:  23%|██▎       | 460/2000 [01:53<05:47,  4.43it/s, Loss=1.6256691, Gaussian number=182686, print grad=0.14985758066177368, Depth Loss=0.0000000]
Training progress:  23%|██▎       | 460/2000 [01:56<05:47,  4.43it/s, Loss=1.9354684, Gaussian number=182686, print grad=0.15382114052772522, Depth Loss=0.0000000]
Training progress:  24%|██▎       | 470/2000 [01:56<05:37,  4.53it/s, Loss=1.9354684, Gaussian number=182686, print grad=0.15382114052772522, Depth Loss=0.0000000]
Training progress:  24%|██▎       | 470/2000 [01:58<05:37,  4.53it/s, Loss=1.2566985, Gaussian number=182686, print grad=0.15834493935108185, Depth Loss=0.0000000]
Training progress:  24%|██▍       | 480/2000 [01:58<05:30,  4.60it/s, Loss=1.2566985, Gaussian number=182686, print grad=0.15834493935108185, Depth Loss=0.0000000]
Training progress:  24%|██▍       | 480/2000 [02:00<05:30,  4.60it/s, Loss=1.3950403, Gaussian number=182686, print grad=0.16243383288383484, Depth Loss=0.0000000]
Training progress:  24%|██▍       | 490/2000 [02:00<05:41,  4.42it/s, Loss=1.3950403, Gaussian number=182686, print grad=0.16243383288383484, Depth Loss=0.0000000]
Training progress:  24%|██▍       | 490/2000 [02:03<05:41,  4.42it/s, Loss=1.0955454, Gaussian number=182686, print grad=0.16663724184036255, Depth Loss=0.0000000]
Training progress:  25%|██▌       | 500/2000 [02:03<05:45,  4.35it/s, Loss=1.0955454, Gaussian number=182686, print grad=0.16663724184036255, Depth Loss=0.0000000]
Training progress:  25%|██▌       | 500/2000 [02:08<05:45,  4.35it/s, Loss=1.2923431, Gaussian number=182686, print grad=0.1708473563194275, Depth Loss=0.0000000] 
Training progress:  26%|██▌       | 510/2000 [02:08<07:52,  3.15it/s, Loss=1.2923431, Gaussian number=182686, print grad=0.1708473563194275, Depth Loss=0.0000000]
Training progress:  26%|██▌       | 510/2000 [02:10<07:52,  3.15it/s, Loss=1.3254993, Gaussian number=182686, print grad=0.17529231309890747, Depth Loss=0.0000000]
Training progress:  26%|██▌       | 520/2000 [02:10<07:01,  3.51it/s, Loss=1.3254993, Gaussian number=182686, print grad=0.17529231309890747, Depth Loss=0.0000000]
Training progress:  26%|██▌       | 520/2000 [02:12<07:01,  3.51it/s, Loss=1.0497196, Gaussian number=182686, print grad=0.17913818359375, Depth Loss=0.0000000]   
Training progress:  26%|██▋       | 530/2000 [02:12<06:25,  3.81it/s, Loss=1.0497196, Gaussian number=182686, print grad=0.17913818359375, Depth Loss=0.0000000]
Training progress:  26%|██▋       | 530/2000 [02:14<06:25,  3.81it/s, Loss=1.4123449, Gaussian number=182686, print grad=0.18338444828987122, Depth Loss=0.0000000]
Training progress:  27%|██▋       | 540/2000 [02:14<05:59,  4.06it/s, Loss=1.4123449, Gaussian number=182686, print grad=0.18338444828987122, Depth Loss=0.0000000]
Training progress:  27%|██▋       | 540/2000 [02:16<05:59,  4.06it/s, Loss=1.3523118, Gaussian number=182686, print grad=0.1879311501979828, Depth Loss=0.0000000] 
Training progress:  28%|██▊       | 550/2000 [02:16<05:42,  4.24it/s, Loss=1.3523118, Gaussian number=182686, print grad=0.1879311501979828, Depth Loss=0.0000000]
Training progress:  28%|██▊       | 550/2000 [02:18<05:42,  4.24it/s, Loss=1.0836232, Gaussian number=182686, print grad=0.19209933280944824, Depth Loss=0.0000000]
Training progress:  28%|██▊       | 560/2000 [02:18<05:30,  4.36it/s, Loss=1.0836232, Gaussian number=182686, print grad=0.19209933280944824, Depth Loss=0.0000000]
Training progress:  28%|██▊       | 560/2000 [02:20<05:30,  4.36it/s, Loss=1.4455932, Gaussian number=182686, print grad=0.19680801033973694, Depth Loss=0.0000000]
Training progress:  28%|██▊       | 570/2000 [02:20<05:22,  4.43it/s, Loss=1.4455932, Gaussian number=182686, print grad=0.19680801033973694, Depth Loss=0.0000000]
Training progress:  28%|██▊       | 570/2000 [02:23<05:22,  4.43it/s, Loss=1.2455485, Gaussian number=182686, print grad=0.20112048089504242, Depth Loss=0.0000000]
Training progress:  29%|██▉       | 580/2000 [02:23<05:15,  4.50it/s, Loss=1.2455485, Gaussian number=182686, print grad=0.20112048089504242, Depth Loss=0.0000000]
Training progress:  29%|██▉       | 580/2000 [02:25<05:15,  4.50it/s, Loss=1.4322298, Gaussian number=182686, print grad=0.20564527809619904, Depth Loss=0.0000000]
Training progress:  30%|██▉       | 590/2000 [02:25<05:04,  4.63it/s, Loss=1.4322298, Gaussian number=182686, print grad=0.20564527809619904, Depth Loss=0.0000000]
Training progress:  30%|██▉       | 590/2000 [02:27<05:04,  4.63it/s, Loss=1.4377173, Gaussian number=182686, print grad=0.20983724296092987, Depth Loss=0.0000000]
Training progress:  30%|███       | 600/2000 [02:27<04:52,  4.78it/s, Loss=1.4377173, Gaussian number=182686, print grad=0.20983724296092987, Depth Loss=0.0000000]
Training progress:  30%|███       | 600/2000 [02:31<04:52,  4.78it/s, Loss=1.4578787, Gaussian number=306434, print grad=0.003074632491916418, Depth Loss=0.0000000]
Training progress:  30%|███       | 610/2000 [02:31<06:23,  3.63it/s, Loss=1.4578787, Gaussian number=306434, print grad=0.003074632491916418, Depth Loss=0.0000000]
Training progress:  30%|███       | 610/2000 [02:33<06:23,  3.63it/s, Loss=1.8792324, Gaussian number=306434, print grad=0.006930652540177107, Depth Loss=0.0000000]
Training progress:  31%|███       | 620/2000 [02:33<05:44,  4.01it/s, Loss=1.8792324, Gaussian number=306434, print grad=0.006930652540177107, Depth Loss=0.0000000]
Training progress:  31%|███       | 620/2000 [02:35<05:44,  4.01it/s, Loss=1.3821207, Gaussian number=306434, print grad=0.009953487664461136, Depth Loss=0.0000000]
Training progress:  32%|███▏      | 630/2000 [02:35<05:17,  4.32it/s, Loss=1.3821207, Gaussian number=306434, print grad=0.009953487664461136, Depth Loss=0.0000000]
Training progress:  32%|███▏      | 630/2000 [02:36<05:17,  4.32it/s, Loss=1.4257240, Gaussian number=306434, print grad=0.013765965588390827, Depth Loss=0.0000000]
Training progress:  32%|███▏      | 640/2000 [02:36<04:54,  4.61it/s, Loss=1.4257240, Gaussian number=306434, print grad=0.013765965588390827, Depth Loss=0.0000000]
Training progress:  32%|███▏      | 640/2000 [02:38<04:54,  4.61it/s, Loss=1.5686417, Gaussian number=306434, print grad=0.016640285030007362, Depth Loss=0.0000000]
Training progress:  32%|███▎      | 650/2000 [02:38<04:36,  4.88it/s, Loss=1.5686417, Gaussian number=306434, print grad=0.016640285030007362, Depth Loss=0.0000000]
Training progress:  32%|███▎      | 650/2000 [02:40<04:36,  4.88it/s, Loss=1.5756312, Gaussian number=306434, print grad=0.02006915770471096, Depth Loss=0.0000000] 
Training progress:  33%|███▎      | 660/2000 [02:40<04:23,  5.08it/s, Loss=1.5756312, Gaussian number=306434, print grad=0.02006915770471096, Depth Loss=0.0000000]
Training progress:  33%|███▎      | 660/2000 [02:42<04:23,  5.08it/s, Loss=1.3650536, Gaussian number=306434, print grad=0.0231406819075346, Depth Loss=0.0000000] 
Training progress:  34%|███▎      | 670/2000 [02:42<04:13,  5.24it/s, Loss=1.3650536, Gaussian number=306434, print grad=0.0231406819075346, Depth Loss=0.0000000]
Training progress:  34%|███▎      | 670/2000 [02:44<04:13,  5.24it/s, Loss=1.2409620, Gaussian number=306434, print grad=0.02654031105339527, Depth Loss=0.0000000]
Training progress:  34%|███▍      | 680/2000 [02:44<04:06,  5.35it/s, Loss=1.2409620, Gaussian number=306434, print grad=0.02654031105339527, Depth Loss=0.0000000]
Training progress:  34%|███▍      | 680/2000 [02:45<04:06,  5.35it/s, Loss=1.4504663, Gaussian number=306434, print grad=0.029714876785874367, Depth Loss=0.0000000]
Training progress:  34%|███▍      | 690/2000 [02:45<04:00,  5.44it/s, Loss=1.4504663, Gaussian number=306434, print grad=0.029714876785874367, Depth Loss=0.0000000]
Training progress:  34%|███▍      | 690/2000 [02:47<04:00,  5.44it/s, Loss=1.4362916, Gaussian number=306434, print grad=0.03282853588461876, Depth Loss=0.0000000] 
Training progress:  35%|███▌      | 700/2000 [02:47<03:56,  5.50it/s, Loss=1.4362916, Gaussian number=306434, print grad=0.03282853588461876, Depth Loss=0.0000000]
Training progress:  35%|███▌      | 700/2000 [02:51<03:56,  5.50it/s, Loss=1.5133254, Gaussian number=481289, print grad=0.002081222366541624, Depth Loss=0.0000000]
Training progress:  36%|███▌      | 710/2000 [02:51<05:25,  3.96it/s, Loss=1.5133254, Gaussian number=481289, print grad=0.002081222366541624, Depth Loss=0.0000000]
Training progress:  36%|███▌      | 710/2000 [02:53<05:25,  3.96it/s, Loss=1.3362444, Gaussian number=481289, print grad=0.004453418776392937, Depth Loss=0.0000000]
Training progress:  36%|███▌      | 720/2000 [02:53<04:54,  4.34it/s, Loss=1.3362444, Gaussian number=481289, print grad=0.004453418776392937, Depth Loss=0.0000000]
Training progress:  36%|███▌      | 720/2000 [02:55<04:54,  4.34it/s, Loss=1.6927715, Gaussian number=481289, print grad=0.006672450341284275, Depth Loss=0.0000000]
Training progress:  36%|███▋      | 730/2000 [02:55<04:32,  4.65it/s, Loss=1.6927715, Gaussian number=481289, print grad=0.006672450341284275, Depth Loss=0.0000000]
Training progress:  36%|███▋      | 730/2000 [02:57<04:32,  4.65it/s, Loss=1.9565945, Gaussian number=481289, print grad=0.009395971894264221, Depth Loss=0.0000000]
Training progress:  37%|███▋      | 740/2000 [02:57<04:17,  4.90it/s, Loss=1.9565945, Gaussian number=481289, print grad=0.009395971894264221, Depth Loss=0.0000000]
Training progress:  37%|███▋      | 740/2000 [02:58<04:17,  4.90it/s, Loss=1.3919341, Gaussian number=481289, print grad=0.011912481859326363, Depth Loss=0.0000000]
Training progress:  38%|███▊      | 750/2000 [02:58<04:05,  5.09it/s, Loss=1.3919341, Gaussian number=481289, print grad=0.011912481859326363, Depth Loss=0.0000000]
Training progress:  38%|███▊      | 750/2000 [03:00<04:05,  5.09it/s, Loss=1.3246341, Gaussian number=481289, print grad=0.014252985827624798, Depth Loss=0.0000000]
Training progress:  38%|███▊      | 760/2000 [03:00<03:56,  5.24it/s, Loss=1.3246341, Gaussian number=481289, print grad=0.014252985827624798, Depth Loss=0.0000000]
Training progress:  38%|███▊      | 760/2000 [03:02<03:56,  5.24it/s, Loss=1.1659858, Gaussian number=481289, print grad=0.01667039841413498, Depth Loss=0.0000000] 
Training progress:  38%|███▊      | 770/2000 [03:02<03:50,  5.34it/s, Loss=1.1659858, Gaussian number=481289, print grad=0.01667039841413498, Depth Loss=0.0000000]
Training progress:  38%|███▊      | 770/2000 [03:04<03:50,  5.34it/s, Loss=1.5767104, Gaussian number=481289, print grad=0.018969891592860222, Depth Loss=0.0000000]
Training progress:  39%|███▉      | 780/2000 [03:04<03:45,  5.42it/s, Loss=1.5767104, Gaussian number=481289, print grad=0.018969891592860222, Depth Loss=0.0000000]
Training progress:  39%|███▉      | 780/2000 [03:06<03:45,  5.42it/s, Loss=1.7685901, Gaussian number=481289, print grad=0.02126055210828781, Depth Loss=0.0000000] 
Training progress:  40%|███▉      | 790/2000 [03:06<03:41,  5.47it/s, Loss=1.7685901, Gaussian number=481289, print grad=0.02126055210828781, Depth Loss=0.0000000]
Training progress:  40%|███▉      | 790/2000 [03:07<03:41,  5.47it/s, Loss=1.5446834, Gaussian number=481289, print grad=0.023718271404504776, Depth Loss=0.0000000]
Training progress:  40%|████      | 800/2000 [03:07<03:37,  5.51it/s, Loss=1.5446834, Gaussian number=481289, print grad=0.023718271404504776, Depth Loss=0.0000000]
Training progress:  40%|████      | 800/2000 [03:12<03:37,  5.51it/s, Loss=1.8300785, Gaussian number=687089, print grad=0.0018308728467673063, Depth Loss=0.0000000]
Training progress:  40%|████      | 810/2000 [03:12<05:01,  3.94it/s, Loss=1.8300785, Gaussian number=687089, print grad=0.0018308728467673063, Depth Loss=0.0000000]
Training progress:  40%|████      | 810/2000 [03:13<05:01,  3.94it/s, Loss=1.6328351, Gaussian number=687089, print grad=0.003736506449058652, Depth Loss=0.0000000] 
Training progress:  41%|████      | 820/2000 [03:13<04:33,  4.32it/s, Loss=1.6328351, Gaussian number=687089, print grad=0.003736506449058652, Depth Loss=0.0000000]
Training progress:  41%|████      | 820/2000 [03:15<04:33,  4.32it/s, Loss=1.3402136, Gaussian number=687089, print grad=0.00599499698728323, Depth Loss=0.0000000] 
Training progress:  42%|████▏     | 830/2000 [03:15<04:12,  4.64it/s, Loss=1.3402136, Gaussian number=687089, print grad=0.00599499698728323, Depth Loss=0.0000000]
Training progress:  42%|████▏     | 830/2000 [03:17<04:12,  4.64it/s, Loss=1.3749233, Gaussian number=687089, print grad=0.007974694482982159, Depth Loss=0.0000000]
Training progress:  42%|████▏     | 840/2000 [03:17<03:57,  4.88it/s, Loss=1.3749233, Gaussian number=687089, print grad=0.007974694482982159, Depth Loss=0.0000000]
Training progress:  42%|████▏     | 840/2000 [03:19<03:57,  4.88it/s, Loss=1.3340454, Gaussian number=687089, print grad=0.00997951254248619, Depth Loss=0.0000000] 
Training progress:  42%|████▎     | 850/2000 [03:19<03:46,  5.07it/s, Loss=1.3340454, Gaussian number=687089, print grad=0.00997951254248619, Depth Loss=0.0000000]
Training progress:  42%|████▎     | 850/2000 [03:21<03:46,  5.07it/s, Loss=1.4031192, Gaussian number=687089, print grad=0.011794106103479862, Depth Loss=0.0000000]
Training progress:  43%|████▎     | 860/2000 [03:21<03:38,  5.21it/s, Loss=1.4031192, Gaussian number=687089, print grad=0.011794106103479862, Depth Loss=0.0000000]
Training progress:  43%|████▎     | 860/2000 [03:22<03:38,  5.21it/s, Loss=1.5762694, Gaussian number=687089, print grad=0.013631947338581085, Depth Loss=0.0000000]
Training progress:  44%|████▎     | 870/2000 [03:22<03:32,  5.32it/s, Loss=1.5762694, Gaussian number=687089, print grad=0.013631947338581085, Depth Loss=0.0000000]
Training progress:  44%|████▎     | 870/2000 [03:24<03:32,  5.32it/s, Loss=1.4531565, Gaussian number=687089, print grad=0.015463241375982761, Depth Loss=0.0000000]
Training progress:  44%|████▍     | 880/2000 [03:24<03:27,  5.40it/s, Loss=1.4531565, Gaussian number=687089, print grad=0.015463241375982761, Depth Loss=0.0000000]
Training progress:  44%|████▍     | 880/2000 [03:26<03:27,  5.40it/s, Loss=1.1463075, Gaussian number=687089, print grad=0.017340658232569695, Depth Loss=0.0000000]
Training progress:  44%|████▍     | 890/2000 [03:26<03:24,  5.44it/s, Loss=1.1463075, Gaussian number=687089, print grad=0.017340658232569695, Depth Loss=0.0000000]
Training progress:  44%|████▍     | 890/2000 [03:28<03:24,  5.44it/s, Loss=1.5064662, Gaussian number=687089, print grad=0.019125012680888176, Depth Loss=0.0000000]
Training progress:  45%|████▌     | 900/2000 [03:28<03:21,  5.47it/s, Loss=1.5064662, Gaussian number=687089, print grad=0.019125012680888176, Depth Loss=0.0000000]
Training progress:  45%|████▌     | 900/2000 [03:32<03:21,  5.47it/s, Loss=1.4406259, Gaussian number=936500, print grad=0.0014999289996922016, Depth Loss=0.0000000]
Training progress:  46%|████▌     | 910/2000 [03:32<04:38,  3.91it/s, Loss=1.4406259, Gaussian number=936500, print grad=0.0014999289996922016, Depth Loss=0.0000000]
Training progress:  46%|████▌     | 910/2000 [03:34<04:38,  3.91it/s, Loss=1.5690962, Gaussian number=936500, print grad=0.0028760109562426805, Depth Loss=0.0000000]
Training progress:  46%|████▌     | 920/2000 [03:34<04:12,  4.28it/s, Loss=1.5690962, Gaussian number=936500, print grad=0.0028760109562426805, Depth Loss=0.0000000]
Training progress:  46%|████▌     | 920/2000 [03:36<04:12,  4.28it/s, Loss=1.6765838, Gaussian number=936500, print grad=0.0046368371695280075, Depth Loss=0.0000000]
Training progress:  46%|████▋     | 930/2000 [03:36<03:52,  4.60it/s, Loss=1.6765838, Gaussian number=936500, print grad=0.0046368371695280075, Depth Loss=0.0000000]
Training progress:  46%|████▋     | 930/2000 [03:37<03:52,  4.60it/s, Loss=1.4505775, Gaussian number=936500, print grad=0.0061414809897542, Depth Loss=0.0000000]   
Training progress:  47%|████▋     | 940/2000 [03:37<03:38,  4.84it/s, Loss=1.4505775, Gaussian number=936500, print grad=0.0061414809897542, Depth Loss=0.0000000]
Training progress:  47%|████▋     | 940/2000 [03:39<03:38,  4.84it/s, Loss=1.4700829, Gaussian number=936500, print grad=0.007723551243543625, Depth Loss=0.0000000]
Training progress:  48%|████▊     | 950/2000 [03:39<03:29,  5.02it/s, Loss=1.4700829, Gaussian number=936500, print grad=0.007723551243543625, Depth Loss=0.0000000]
Training progress:  48%|████▊     | 950/2000 [03:41<03:29,  5.02it/s, Loss=1.3944217, Gaussian number=936500, print grad=0.009175625629723072, Depth Loss=0.0000000]
Training progress:  48%|████▊     | 960/2000 [03:41<03:21,  5.16it/s, Loss=1.3944217, Gaussian number=936500, print grad=0.009175625629723072, Depth Loss=0.0000000]
Training progress:  48%|████▊     | 960/2000 [03:43<03:21,  5.16it/s, Loss=1.8014289, Gaussian number=936500, print grad=0.010748758912086487, Depth Loss=0.0000000]
Training progress:  48%|████▊     | 970/2000 [03:43<03:15,  5.26it/s, Loss=1.8014289, Gaussian number=936500, print grad=0.010748758912086487, Depth Loss=0.0000000]
Training progress:  48%|████▊     | 970/2000 [03:45<03:15,  5.26it/s, Loss=1.1215076, Gaussian number=936500, print grad=0.01221542339771986, Depth Loss=0.0000000] 
Training progress:  49%|████▉     | 980/2000 [03:45<03:11,  5.33it/s, Loss=1.1215076, Gaussian number=936500, print grad=0.01221542339771986, Depth Loss=0.0000000]
Training progress:  49%|████▉     | 980/2000 [03:47<03:11,  5.33it/s, Loss=1.1766099, Gaussian number=936500, print grad=0.013583613559603691, Depth Loss=0.0000000]
Training progress:  50%|████▉     | 990/2000 [03:47<03:14,  5.19it/s, Loss=1.1766099, Gaussian number=936500, print grad=0.013583613559603691, Depth Loss=0.0000000]
Training progress:  50%|████▉     | 990/2000 [03:49<03:14,  5.19it/s, Loss=1.4642619, Gaussian number=936500, print grad=0.014833958819508553, Depth Loss=0.0000000]
Training progress:  50%|█████     | 1000/2000 [03:49<03:08,  5.29it/s, Loss=1.4642619, Gaussian number=936500, print grad=0.014833958819508553, Depth Loss=0.0000000]
Training progress:  50%|█████     | 1000/2000 [03:53<03:08,  5.29it/s, Loss=1.6350279, Gaussian number=1224798, print grad=0.001034929882735014, Depth Loss=0.0000000]
Training progress:  50%|█████     | 1010/2000 [03:53<04:19,  3.81it/s, Loss=1.6350279, Gaussian number=1224798, print grad=0.001034929882735014, Depth Loss=0.0000000]
Training progress:  50%|█████     | 1010/2000 [03:55<04:19,  3.81it/s, Loss=2.0109773, Gaussian number=1224798, print grad=0.00248338608071208, Depth Loss=0.0000000] 
Training progress:  51%|█████     | 1020/2000 [03:55<03:55,  4.17it/s, Loss=2.0109773, Gaussian number=1224798, print grad=0.00248338608071208, Depth Loss=0.0000000]
Training progress:  51%|█████     | 1020/2000 [03:57<03:55,  4.17it/s, Loss=1.7117825, Gaussian number=1224798, print grad=0.0038026082329452038, Depth Loss=0.0000000]
Training progress:  52%|█████▏    | 1030/2000 [03:57<03:37,  4.47it/s, Loss=1.7117825, Gaussian number=1224798, print grad=0.0038026082329452038, Depth Loss=0.0000000]
Training progress:  52%|█████▏    | 1030/2000 [03:58<03:37,  4.47it/s, Loss=1.6513308, Gaussian number=1224798, print grad=0.0052527473308146, Depth Loss=0.0000000]   
Training progress:  52%|█████▏    | 1040/2000 [03:58<03:23,  4.72it/s, Loss=1.6513308, Gaussian number=1224798, print grad=0.0052527473308146, Depth Loss=0.0000000]
Training progress:  52%|█████▏    | 1040/2000 [04:00<03:23,  4.72it/s, Loss=1.4507385, Gaussian number=1224798, print grad=0.006405343767255545, Depth Loss=0.0000000]
Training progress:  52%|█████▎    | 1050/2000 [04:00<03:13,  4.90it/s, Loss=1.4507385, Gaussian number=1224798, print grad=0.006405343767255545, Depth Loss=0.0000000]
Training progress:  52%|█████▎    | 1050/2000 [04:02<03:13,  4.90it/s, Loss=1.4230951, Gaussian number=1224798, print grad=0.007670745253562927, Depth Loss=0.0000000]
Training progress:  53%|█████▎    | 1060/2000 [04:02<03:06,  5.05it/s, Loss=1.4230951, Gaussian number=1224798, print grad=0.007670745253562927, Depth Loss=0.0000000]
Training progress:  53%|█████▎    | 1060/2000 [04:04<03:06,  5.05it/s, Loss=1.1568730, Gaussian number=1224798, print grad=0.00901837833225727, Depth Loss=0.0000000] 
Training progress:  54%|█████▎    | 1070/2000 [04:04<03:00,  5.15it/s, Loss=1.1568730, Gaussian number=1224798, print grad=0.00901837833225727, Depth Loss=0.0000000]
Training progress:  54%|█████▎    | 1070/2000 [04:06<03:00,  5.15it/s, Loss=1.1222808, Gaussian number=1224798, print grad=0.010154212825000286, Depth Loss=0.0000000]
Training progress:  54%|█████▍    | 1080/2000 [04:06<02:56,  5.22it/s, Loss=1.1222808, Gaussian number=1224798, print grad=0.010154212825000286, Depth Loss=0.0000000]
Training progress:  54%|█████▍    | 1080/2000 [04:08<02:56,  5.22it/s, Loss=1.4794460, Gaussian number=1224798, print grad=0.01139485090970993, Depth Loss=0.0000000] 
Training progress:  55%|█████▍    | 1090/2000 [04:08<02:52,  5.28it/s, Loss=1.4794460, Gaussian number=1224798, print grad=0.01139485090970993, Depth Loss=0.0000000]
Training progress:  55%|█████▍    | 1090/2000 [04:10<02:52,  5.28it/s, Loss=1.4880258, Gaussian number=1224798, print grad=0.012565040960907936, Depth Loss=0.0000000]
Training progress:  55%|█████▌    | 1100/2000 [04:10<02:49,  5.32it/s, Loss=1.4880258, Gaussian number=1224798, print grad=0.012565040960907936, Depth Loss=0.0000000]
Training progress:  55%|█████▌    | 1100/2000 [04:11<02:49,  5.32it/s, Loss=1.7719260, Gaussian number=1552553, print grad=0.000910684815607965, Depth Loss=0.0000000]
Training progress:  56%|█████▌    | 1110/2000 [04:11<02:48,  5.29it/s, Loss=1.7719260, Gaussian number=1552553, print grad=0.000910684815607965, Depth Loss=0.0000000]
Training progress:  56%|█████▌    | 1110/2000 [04:13<02:48,  5.29it/s, Loss=1.8187567, Gaussian number=1552553, print grad=0.002028360264375806, Depth Loss=0.0000000]
Training progress:  56%|█████▌    | 1120/2000 [04:13<02:46,  5.27it/s, Loss=1.8187567, Gaussian number=1552553, print grad=0.002028360264375806, Depth Loss=0.0000000]
Training progress:  56%|█████▌    | 1120/2000 [04:15<02:46,  5.27it/s, Loss=1.3733082, Gaussian number=1552553, print grad=0.0032514724880456924, Depth Loss=0.0000000]
Training progress:  56%|█████▋    | 1130/2000 [04:15<02:45,  5.27it/s, Loss=1.3733082, Gaussian number=1552553, print grad=0.0032514724880456924, Depth Loss=0.0000000]
Training progress:  56%|█████▋    | 1130/2000 [04:17<02:45,  5.27it/s, Loss=1.5817305, Gaussian number=1552553, print grad=0.004386081360280514, Depth Loss=0.0000000] 
Training progress:  57%|█████▋    | 1140/2000 [04:17<02:43,  5.25it/s, Loss=1.5817305, Gaussian number=1552553, print grad=0.004386081360280514, Depth Loss=0.0000000]
Training progress:  57%|█████▋    | 1140/2000 [04:19<02:43,  5.25it/s, Loss=1.1980784, Gaussian number=1552553, print grad=0.005579040851444006, Depth Loss=0.0000000]
Training progress:  57%|█████▊    | 1150/2000 [04:19<02:41,  5.25it/s, Loss=1.1980784, Gaussian number=1552553, print grad=0.005579040851444006, Depth Loss=0.0000000]
Training progress:  57%|█████▊    | 1150/2000 [04:21<02:41,  5.25it/s, Loss=1.2418561, Gaussian number=1552553, print grad=0.006549391429871321, Depth Loss=0.0000000]
Training progress:  58%|█████▊    | 1160/2000 [04:21<02:40,  5.25it/s, Loss=1.2418561, Gaussian number=1552553, print grad=0.006549391429871321, Depth Loss=0.0000000]
Training progress:  58%|█████▊    | 1160/2000 [04:23<02:40,  5.25it/s, Loss=1.4263316, Gaussian number=1552553, print grad=0.007592912297695875, Depth Loss=0.0000000]
Training progress:  58%|█████▊    | 1170/2000 [04:23<02:37,  5.26it/s, Loss=1.4263316, Gaussian number=1552553, print grad=0.007592912297695875, Depth Loss=0.0000000]
Training progress:  58%|█████▊    | 1170/2000 [04:25<02:37,  5.26it/s, Loss=1.5255710, Gaussian number=1552553, print grad=0.008657865226268768, Depth Loss=0.0000000]
Training progress:  59%|█████▉    | 1180/2000 [04:25<02:35,  5.26it/s, Loss=1.5255710, Gaussian number=1552553, print grad=0.008657865226268768, Depth Loss=0.0000000]
Training progress:  59%|█████▉    | 1180/2000 [04:27<02:35,  5.26it/s, Loss=1.4049120, Gaussian number=1552553, print grad=0.009768284857273102, Depth Loss=0.0000000]
Training progress:  60%|█████▉    | 1190/2000 [04:27<02:34,  5.25it/s, Loss=1.4049120, Gaussian number=1552553, print grad=0.009768284857273102, Depth Loss=0.0000000]
Training progress:  60%|█████▉    | 1190/2000 [04:29<02:34,  5.25it/s, Loss=1.5830701, Gaussian number=1552553, print grad=0.01068821456283331, Depth Loss=0.0000000] 
Training progress:  60%|██████    | 1200/2000 [04:29<02:32,  5.25it/s, Loss=1.5830701, Gaussian number=1552553, print grad=0.01068821456283331, Depth Loss=0.0000000]
Training progress:  60%|██████    | 1200/2000 [04:31<02:32,  5.25it/s, Loss=1.3696805, Gaussian number=1918084, print grad=0.0008686811779625714, Depth Loss=0.0000000]
Training progress:  60%|██████    | 1210/2000 [04:31<02:31,  5.21it/s, Loss=1.3696805, Gaussian number=1918084, print grad=0.0008686811779625714, Depth Loss=0.0000000]
Training progress:  60%|██████    | 1210/2000 [04:32<02:31,  5.21it/s, Loss=1.3977625, Gaussian number=1918084, print grad=0.001928137382492423, Depth Loss=0.0000000] 
Training progress:  61%|██████    | 1220/2000 [04:32<02:29,  5.22it/s, Loss=1.3977625, Gaussian number=1918084, print grad=0.001928137382492423, Depth Loss=0.0000000]
Training progress:  61%|██████    | 1220/2000 [04:34<02:29,  5.22it/s, Loss=1.2928230, Gaussian number=1918084, print grad=0.0029265594203025103, Depth Loss=0.0000000]
Training progress:  62%|██████▏   | 1230/2000 [04:34<02:27,  5.22it/s, Loss=1.2928230, Gaussian number=1918084, print grad=0.0029265594203025103, Depth Loss=0.0000000]
Training progress:  62%|██████▏   | 1230/2000 [04:36<02:27,  5.22it/s, Loss=1.2700735, Gaussian number=1918084, print grad=0.003897920483723283, Depth Loss=0.0000000] 
Training progress:  62%|██████▏   | 1240/2000 [04:36<02:25,  5.22it/s, Loss=1.2700735, Gaussian number=1918084, print grad=0.003897920483723283, Depth Loss=0.0000000]
Training progress:  62%|██████▏   | 1240/2000 [04:38<02:25,  5.22it/s, Loss=1.2201130, Gaussian number=1918084, print grad=0.004767913371324539, Depth Loss=0.0000000]
Training progress:  62%|██████▎   | 1250/2000 [04:38<02:23,  5.22it/s, Loss=1.2201130, Gaussian number=1918084, print grad=0.004767913371324539, Depth Loss=0.0000000]
Training progress:  62%|██████▎   | 1250/2000 [04:40<02:23,  5.22it/s, Loss=1.1953192, Gaussian number=1918084, print grad=0.005569504573941231, Depth Loss=0.0000000]
Training progress:  63%|██████▎   | 1260/2000 [04:40<02:21,  5.22it/s, Loss=1.1953192, Gaussian number=1918084, print grad=0.005569504573941231, Depth Loss=0.0000000]
Training progress:  63%|██████▎   | 1260/2000 [04:42<02:21,  5.22it/s, Loss=1.4297445, Gaussian number=1918084, print grad=0.006482444703578949, Depth Loss=0.0000000]
Training progress:  64%|██████▎   | 1270/2000 [04:42<02:19,  5.23it/s, Loss=1.4297445, Gaussian number=1918084, print grad=0.006482444703578949, Depth Loss=0.0000000]
Training progress:  64%|██████▎   | 1270/2000 [04:44<02:19,  5.23it/s, Loss=1.3907294, Gaussian number=1918084, print grad=0.0072886087000370026, Depth Loss=0.0000000]
Training progress:  64%|██████▍   | 1280/2000 [04:44<02:18,  5.20it/s, Loss=1.3907294, Gaussian number=1918084, print grad=0.0072886087000370026, Depth Loss=0.0000000]
Training progress:  64%|██████▍   | 1280/2000 [04:46<02:18,  5.20it/s, Loss=1.1045836, Gaussian number=1918084, print grad=0.008251374587416649, Depth Loss=0.0000000] 
Training progress:  64%|██████▍   | 1290/2000 [04:46<02:16,  5.21it/s, Loss=1.1045836, Gaussian number=1918084, print grad=0.008251374587416649, Depth Loss=0.0000000]
Training progress:  64%|██████▍   | 1290/2000 [04:48<02:16,  5.21it/s, Loss=1.7679400, Gaussian number=1918084, print grad=0.00909147784113884, Depth Loss=0.0000000] 
Training progress:  65%|██████▌   | 1300/2000 [04:48<02:14,  5.20it/s, Loss=1.7679400, Gaussian number=1918084, print grad=0.00909147784113884, Depth Loss=0.0000000]
Training progress:  65%|██████▌   | 1300/2000 [04:50<02:14,  5.20it/s, Loss=1.8403887, Gaussian number=2321847, print grad=0.0008067178423516452, Depth Loss=0.0000000]
Training progress:  66%|██████▌   | 1310/2000 [04:50<02:15,  5.11it/s, Loss=1.8403887, Gaussian number=2321847, print grad=0.0008067178423516452, Depth Loss=0.0000000]
Training progress:  66%|██████▌   | 1310/2000 [04:52<02:15,  5.11it/s, Loss=1.9158398, Gaussian number=2321847, print grad=0.001615342334844172, Depth Loss=0.0000000] 
Training progress:  66%|██████▌   | 1320/2000 [04:52<02:13,  5.10it/s, Loss=1.9158398, Gaussian number=2321847, print grad=0.001615342334844172, Depth Loss=0.0000000]
Training progress:  66%|██████▌   | 1320/2000 [04:54<02:13,  5.10it/s, Loss=1.6048200, Gaussian number=2321847, print grad=0.0025404293555766344, Depth Loss=0.0000000]
Training progress:  66%|██████▋   | 1330/2000 [04:54<02:12,  5.06it/s, Loss=1.6048200, Gaussian number=2321847, print grad=0.0025404293555766344, Depth Loss=0.0000000]
Training progress:  66%|██████▋   | 1330/2000 [04:56<02:12,  5.06it/s, Loss=1.5093178, Gaussian number=2321847, print grad=0.0033449637703597546, Depth Loss=0.0000000]
Training progress:  67%|██████▋   | 1340/2000 [04:56<02:10,  5.06it/s, Loss=1.5093178, Gaussian number=2321847, print grad=0.0033449637703597546, Depth Loss=0.0000000]
Training progress:  67%|██████▋   | 1340/2000 [04:58<02:10,  5.06it/s, Loss=1.9002848, Gaussian number=2321847, print grad=0.004022782202810049, Depth Loss=0.0000000] 
Training progress:  68%|██████▊   | 1350/2000 [04:58<02:08,  5.07it/s, Loss=1.9002848, Gaussian number=2321847, print grad=0.004022782202810049, Depth Loss=0.0000000]
Training progress:  68%|██████▊   | 1350/2000 [05:00<02:08,  5.07it/s, Loss=1.1735367, Gaussian number=2321847, print grad=0.0047763641923666, Depth Loss=0.0000000]  
Training progress:  68%|██████▊   | 1360/2000 [05:00<02:05,  5.08it/s, Loss=1.1735367, Gaussian number=2321847, print grad=0.0047763641923666, Depth Loss=0.0000000]
Training progress:  68%|██████▊   | 1360/2000 [05:02<02:05,  5.08it/s, Loss=2.1513756, Gaussian number=2321847, print grad=0.005614408757537603, Depth Loss=0.0000000]
Training progress:  68%|██████▊   | 1370/2000 [05:02<02:04,  5.07it/s, Loss=2.1513756, Gaussian number=2321847, print grad=0.005614408757537603, Depth Loss=0.0000000]
Training progress:  68%|██████▊   | 1370/2000 [05:04<02:04,  5.07it/s, Loss=1.4297344, Gaussian number=2321847, print grad=0.006308307871222496, Depth Loss=0.0000000]
Training progress:  69%|██████▉   | 1380/2000 [05:04<02:02,  5.08it/s, Loss=1.4297344, Gaussian number=2321847, print grad=0.006308307871222496, Depth Loss=0.0000000]
Training progress:  69%|██████▉   | 1380/2000 [05:06<02:02,  5.08it/s, Loss=1.2121356, Gaussian number=2321847, print grad=0.007049154955893755, Depth Loss=0.0000000]
Training progress:  70%|██████▉   | 1390/2000 [05:06<02:00,  5.07it/s, Loss=1.2121356, Gaussian number=2321847, print grad=0.007049154955893755, Depth Loss=0.0000000]
Training progress:  70%|██████▉   | 1390/2000 [05:08<02:00,  5.07it/s, Loss=1.3601147, Gaussian number=2321847, print grad=0.007757899817079306, Depth Loss=0.0000000]
Training progress:  70%|███████   | 1400/2000 [05:08<01:58,  5.08it/s, Loss=1.3601147, Gaussian number=2321847, print grad=0.007757899817079306, Depth Loss=0.0000000]
Training progress:  70%|███████   | 1400/2000 [05:10<01:58,  5.08it/s, Loss=1.7635086, Gaussian number=2738045, print grad=0.0006921540480107069, Depth Loss=0.0000000]
Training progress:  70%|███████   | 1410/2000 [05:10<01:57,  5.03it/s, Loss=1.7635086, Gaussian number=2738045, print grad=0.0006921540480107069, Depth Loss=0.0000000]
Training progress:  70%|███████   | 1410/2000 [05:12<01:57,  5.03it/s, Loss=1.4817545, Gaussian number=2738045, print grad=0.0014151937793940306, Depth Loss=0.0000000]
Training progress:  71%|███████   | 1420/2000 [05:12<01:56,  4.99it/s, Loss=1.4817545, Gaussian number=2738045, print grad=0.0014151937793940306, Depth Loss=0.0000000]
Training progress:  71%|███████   | 1420/2000 [05:14<01:56,  4.99it/s, Loss=1.7117440, Gaussian number=2738045, print grad=0.002246890217065811, Depth Loss=0.0000000] 
Training progress:  72%|███████▏  | 1430/2000 [05:14<01:54,  4.97it/s, Loss=1.7117440, Gaussian number=2738045, print grad=0.002246890217065811, Depth Loss=0.0000000]
Training progress:  72%|███████▏  | 1430/2000 [05:16<01:54,  4.97it/s, Loss=1.4142397, Gaussian number=2738045, print grad=0.0028869935777038336, Depth Loss=0.0000000]
Training progress:  72%|███████▏  | 1440/2000 [05:16<01:53,  4.93it/s, Loss=1.4142397, Gaussian number=2738045, print grad=0.0028869935777038336, Depth Loss=0.0000000]
Training progress:  72%|███████▏  | 1440/2000 [05:18<01:53,  4.93it/s, Loss=1.3055608, Gaussian number=2738045, print grad=0.0036153635010123253, Depth Loss=0.0000000]
Training progress:  72%|███████▎  | 1450/2000 [05:18<01:51,  4.94it/s, Loss=1.3055608, Gaussian number=2738045, print grad=0.0036153635010123253, Depth Loss=0.0000000]
Training progress:  72%|███████▎  | 1450/2000 [05:20<01:51,  4.94it/s, Loss=1.1346966, Gaussian number=2738045, print grad=0.0043038795702159405, Depth Loss=0.0000000]
Training progress:  73%|███████▎  | 1460/2000 [05:20<01:49,  4.94it/s, Loss=1.1346966, Gaussian number=2738045, print grad=0.0043038795702159405, Depth Loss=0.0000000]
Training progress:  73%|███████▎  | 1460/2000 [05:22<01:49,  4.94it/s, Loss=1.5267167, Gaussian number=2738045, print grad=0.004995379131287336, Depth Loss=0.0000000] 
Training progress:  74%|███████▎  | 1470/2000 [05:22<01:47,  4.94it/s, Loss=1.5267167, Gaussian number=2738045, print grad=0.004995379131287336, Depth Loss=0.0000000]
Training progress:  74%|███████▎  | 1470/2000 [05:24<01:47,  4.94it/s, Loss=1.5428171, Gaussian number=2738045, print grad=0.005756028462201357, Depth Loss=0.0000000]
Training progress:  74%|███████▍  | 1480/2000 [05:24<01:46,  4.90it/s, Loss=1.5428171, Gaussian number=2738045, print grad=0.005756028462201357, Depth Loss=0.0000000]
Training progress:  74%|███████▍  | 1480/2000 [05:26<01:46,  4.90it/s, Loss=1.6154255, Gaussian number=2738045, print grad=0.006401674821972847, Depth Loss=0.0000000]
Training progress:  74%|███████▍  | 1490/2000 [05:26<01:44,  4.90it/s, Loss=1.6154255, Gaussian number=2738045, print grad=0.006401674821972847, Depth Loss=0.0000000]
Training progress:  74%|███████▍  | 1490/2000 [05:28<01:44,  4.90it/s, Loss=1.5339115, Gaussian number=2738045, print grad=0.007138403132557869, Depth Loss=0.0000000]
Training progress:  75%|███████▌  | 1500/2000 [05:28<01:41,  4.92it/s, Loss=1.5339115, Gaussian number=2738045, print grad=0.007138403132557869, Depth Loss=0.0000000]
Training progress:  75%|███████▌  | 1500/2000 [05:33<01:41,  4.92it/s, Loss=1.8142744, Gaussian number=3187397, print grad=0.0006677008932456374, Depth Loss=0.0000000]
Training progress:  76%|███████▌  | 1510/2000 [05:33<02:20,  3.49it/s, Loss=1.8142744, Gaussian number=3187397, print grad=0.0006677008932456374, Depth Loss=0.0000000]
Training progress:  76%|███████▌  | 1510/2000 [05:35<02:20,  3.49it/s, Loss=1.8454201, Gaussian number=3187397, print grad=0.0013321662554517388, Depth Loss=0.0000000]
Training progress:  76%|███████▌  | 1520/2000 [05:35<02:06,  3.81it/s, Loss=1.8454201, Gaussian number=3187397, print grad=0.0013321662554517388, Depth Loss=0.0000000]
Training progress:  76%|███████▌  | 1520/2000 [05:37<02:06,  3.81it/s, Loss=1.0351226, Gaussian number=3187397, print grad=0.002084878971800208, Depth Loss=0.0000000] 
Training progress:  76%|███████▋  | 1530/2000 [05:37<01:55,  4.08it/s, Loss=1.0351226, Gaussian number=3187397, print grad=0.002084878971800208, Depth Loss=0.0000000]
Training progress:  76%|███████▋  | 1530/2000 [05:39<01:55,  4.08it/s, Loss=1.3674241, Gaussian number=3187397, print grad=0.002760224277153611, Depth Loss=0.0000000]
Training progress:  77%|███████▋  | 1540/2000 [05:39<01:47,  4.27it/s, Loss=1.3674241, Gaussian number=3187397, print grad=0.002760224277153611, Depth Loss=0.0000000]
Training progress:  77%|███████▋  | 1540/2000 [05:41<01:47,  4.27it/s, Loss=1.6510405, Gaussian number=3187397, print grad=0.003444024594500661, Depth Loss=0.0000000]
Training progress:  78%|███████▊  | 1550/2000 [05:41<01:41,  4.43it/s, Loss=1.6510405, Gaussian number=3187397, print grad=0.003444024594500661, Depth Loss=0.0000000]
Training progress:  78%|███████▊  | 1550/2000 [05:43<01:41,  4.43it/s, Loss=1.6870069, Gaussian number=3187397, print grad=0.004170124884694815, Depth Loss=0.0000000]
Training progress:  78%|███████▊  | 1560/2000 [05:43<01:36,  4.55it/s, Loss=1.6870069, Gaussian number=3187397, print grad=0.004170124884694815, Depth Loss=0.0000000]
Training progress:  78%|███████▊  | 1560/2000 [05:45<01:36,  4.55it/s, Loss=1.3957497, Gaussian number=3187397, print grad=0.004860894288867712, Depth Loss=0.0000000]
Training progress:  78%|███████▊  | 1570/2000 [05:45<01:32,  4.63it/s, Loss=1.3957497, Gaussian number=3187397, print grad=0.004860894288867712, Depth Loss=0.0000000]
Training progress:  78%|███████▊  | 1570/2000 [05:47<01:32,  4.63it/s, Loss=0.9987090, Gaussian number=3187397, print grad=0.005420294124633074, Depth Loss=0.0000000]
Training progress:  79%|███████▉  | 1580/2000 [05:47<01:28,  4.72it/s, Loss=0.9987090, Gaussian number=3187397, print grad=0.005420294124633074, Depth Loss=0.0000000]
Training progress:  79%|███████▉  | 1580/2000 [05:49<01:28,  4.72it/s, Loss=1.4089781, Gaussian number=3187397, print grad=0.006041104439646006, Depth Loss=0.0000000]
Training progress:  80%|███████▉  | 1590/2000 [05:49<01:26,  4.74it/s, Loss=1.4089781, Gaussian number=3187397, print grad=0.006041104439646006, Depth Loss=0.0000000]
Training progress:  80%|███████▉  | 1590/2000 [05:51<01:26,  4.74it/s, Loss=1.2351855, Gaussian number=3187397, print grad=0.006719543132930994, Depth Loss=0.0000000]
Training progress:  80%|████████  | 1600/2000 [05:51<01:23,  4.78it/s, Loss=1.2351855, Gaussian number=3187397, print grad=0.006719543132930994, Depth Loss=0.0000000]
Training progress:  80%|████████  | 1600/2000 [05:54<01:23,  4.78it/s, Loss=1.8157792, Gaussian number=3655446, print grad=0.0005225086933933198, Depth Loss=0.0000000]
Training progress:  80%|████████  | 1610/2000 [05:54<01:22,  4.72it/s, Loss=1.8157792, Gaussian number=3655446, print grad=0.0005225086933933198, Depth Loss=0.0000000]
Training progress:  80%|████████  | 1610/2000 [05:56<01:22,  4.72it/s, Loss=1.7401628, Gaussian number=3655446, print grad=0.0012675989419221878, Depth Loss=0.0000000]
Training progress:  81%|████████  | 1620/2000 [05:56<01:20,  4.72it/s, Loss=1.7401628, Gaussian number=3655446, print grad=0.0012675989419221878, Depth Loss=0.0000000]
Training progress:  81%|████████  | 1620/2000 [05:58<01:20,  4.72it/s, Loss=1.4019105, Gaussian number=3655446, print grad=0.0019144552061334252, Depth Loss=0.0000000]
Training progress:  82%|████████▏ | 1630/2000 [05:58<01:18,  4.73it/s, Loss=1.4019105, Gaussian number=3655446, print grad=0.0019144552061334252, Depth Loss=0.0000000]
Training progress:  82%|████████▏ | 1630/2000 [06:00<01:18,  4.73it/s, Loss=1.2669221, Gaussian number=3655446, print grad=0.0024720337241888046, Depth Loss=0.0000000]
Training progress:  82%|████████▏ | 1640/2000 [06:00<01:16,  4.73it/s, Loss=1.2669221, Gaussian number=3655446, print grad=0.0024720337241888046, Depth Loss=0.0000000]
Training progress:  82%|████████▏ | 1640/2000 [06:02<01:16,  4.73it/s, Loss=1.4600735, Gaussian number=3655446, print grad=0.003070640843361616, Depth Loss=0.0000000] 
Training progress:  82%|████████▎ | 1650/2000 [06:02<01:14,  4.72it/s, Loss=1.4600735, Gaussian number=3655446, print grad=0.003070640843361616, Depth Loss=0.0000000]
Training progress:  82%|████████▎ | 1650/2000 [06:04<01:14,  4.72it/s, Loss=1.4087983, Gaussian number=3655446, print grad=0.0035962043330073357, Depth Loss=0.0000000]
Training progress:  83%|████████▎ | 1660/2000 [06:04<01:12,  4.71it/s, Loss=1.4087983, Gaussian number=3655446, print grad=0.0035962043330073357, Depth Loss=0.0000000]
Training progress:  83%|████████▎ | 1660/2000 [06:06<01:12,  4.71it/s, Loss=1.2765272, Gaussian number=3655446, print grad=0.004155046306550503, Depth Loss=0.0000000] 
Training progress:  84%|████████▎ | 1670/2000 [06:06<01:09,  4.74it/s, Loss=1.2765272, Gaussian number=3655446, print grad=0.004155046306550503, Depth Loss=0.0000000]
Training progress:  84%|████████▎ | 1670/2000 [06:08<01:09,  4.74it/s, Loss=1.2803493, Gaussian number=3655446, print grad=0.004647838417440653, Depth Loss=0.0000000]
Training progress:  84%|████████▍ | 1680/2000 [06:08<01:07,  4.72it/s, Loss=1.2803493, Gaussian number=3655446, print grad=0.004647838417440653, Depth Loss=0.0000000]
Training progress:  84%|████████▍ | 1680/2000 [06:10<01:07,  4.72it/s, Loss=1.3664947, Gaussian number=3655446, print grad=0.005236747674643993, Depth Loss=0.0000000]
Training progress:  84%|████████▍ | 1690/2000 [06:10<01:05,  4.72it/s, Loss=1.3664947, Gaussian number=3655446, print grad=0.005236747674643993, Depth Loss=0.0000000]
Training progress:  84%|████████▍ | 1690/2000 [06:13<01:05,  4.72it/s, Loss=1.5593736, Gaussian number=3655446, print grad=0.005789304617792368, Depth Loss=0.0000000]
Training progress:  85%|████████▌ | 1700/2000 [06:13<01:03,  4.73it/s, Loss=1.5593736, Gaussian number=3655446, print grad=0.005789304617792368, Depth Loss=0.0000000]
Training progress:  85%|████████▌ | 1700/2000 [06:15<01:03,  4.73it/s, Loss=1.6099345, Gaussian number=4100093, print grad=0.0005820952355861664, Depth Loss=0.0000000]
Training progress:  86%|████████▌ | 1710/2000 [06:15<01:01,  4.71it/s, Loss=1.6099345, Gaussian number=4100093, print grad=0.0005820952355861664, Depth Loss=0.0000000]
Training progress:  86%|████████▌ | 1710/2000 [06:17<01:01,  4.71it/s, Loss=1.7564449, Gaussian number=4100093, print grad=0.0011268967064097524, Depth Loss=0.0000000]
Training progress:  86%|████████▌ | 1720/2000 [06:17<00:59,  4.71it/s, Loss=1.7564449, Gaussian number=4100093, print grad=0.0011268967064097524, Depth Loss=0.0000000]
Training progress:  86%|████████▌ | 1720/2000 [06:19<00:59,  4.71it/s, Loss=1.6430699, Gaussian number=4100093, print grad=0.0016468415269628167, Depth Loss=0.0000000]
Training progress:  86%|████████▋ | 1730/2000 [06:19<00:57,  4.72it/s, Loss=1.6430699, Gaussian number=4100093, print grad=0.0016468415269628167, Depth Loss=0.0000000]
Training progress:  86%|████████▋ | 1730/2000 [06:21<00:57,  4.72it/s, Loss=2.1019497, Gaussian number=4100093, print grad=0.0021718540228903294, Depth Loss=0.0000000]
Training progress:  87%|████████▋ | 1740/2000 [06:21<00:55,  4.67it/s, Loss=2.1019497, Gaussian number=4100093, print grad=0.0021718540228903294, Depth Loss=0.0000000]
Training progress:  87%|████████▋ | 1740/2000 [06:23<00:55,  4.67it/s, Loss=1.9120675, Gaussian number=4100093, print grad=0.0026796332094818354, Depth Loss=0.0000000]
Training progress:  88%|████████▊ | 1750/2000 [06:23<00:53,  4.65it/s, Loss=1.9120675, Gaussian number=4100093, print grad=0.0026796332094818354, Depth Loss=0.0000000]
Training progress:  88%|████████▊ | 1750/2000 [06:25<00:53,  4.65it/s, Loss=1.5784956, Gaussian number=4100093, print grad=0.0032321843318641186, Depth Loss=0.0000000]
Training progress:  88%|████████▊ | 1760/2000 [06:25<00:51,  4.67it/s, Loss=1.5784956, Gaussian number=4100093, print grad=0.0032321843318641186, Depth Loss=0.0000000]
Training progress:  88%|████████▊ | 1760/2000 [06:28<00:51,  4.67it/s, Loss=1.2588169, Gaussian number=4100093, print grad=0.0037226034328341484, Depth Loss=0.0000000]
Training progress:  88%|████████▊ | 1770/2000 [06:28<00:49,  4.68it/s, Loss=1.2588169, Gaussian number=4100093, print grad=0.0037226034328341484, Depth Loss=0.0000000]
Training progress:  88%|████████▊ | 1770/2000 [06:30<00:49,  4.68it/s, Loss=1.3753496, Gaussian number=4100093, print grad=0.004219258204102516, Depth Loss=0.0000000] 
Training progress:  89%|████████▉ | 1780/2000 [06:30<00:47,  4.67it/s, Loss=1.3753496, Gaussian number=4100093, print grad=0.004219258204102516, Depth Loss=0.0000000]
Training progress:  89%|████████▉ | 1780/2000 [06:32<00:47,  4.67it/s, Loss=1.3438254, Gaussian number=4100093, print grad=0.0046608941629529, Depth Loss=0.0000000]  
Training progress:  90%|████████▉ | 1790/2000 [06:32<00:44,  4.69it/s, Loss=1.3438254, Gaussian number=4100093, print grad=0.0046608941629529, Depth Loss=0.0000000]
Training progress:  90%|████████▉ | 1790/2000 [06:34<00:44,  4.69it/s, Loss=1.2844814, Gaussian number=4100093, print grad=0.005204016342759132, Depth Loss=0.0000000]
Training progress:  90%|█████████ | 1800/2000 [06:34<00:42,  4.65it/s, Loss=1.2844814, Gaussian number=4100093, print grad=0.005204016342759132, Depth Loss=0.0000000]
Training progress:  90%|█████████ | 1800/2000 [06:36<00:42,  4.65it/s, Loss=1.6350552, Gaussian number=4572793, print grad=0.0003875159891322255, Depth Loss=0.0000000]
Training progress:  90%|█████████ | 1810/2000 [06:36<00:41,  4.62it/s, Loss=1.6350552, Gaussian number=4572793, print grad=0.0003875159891322255, Depth Loss=0.0000000]
Training progress:  90%|█████████ | 1810/2000 [06:38<00:41,  4.62it/s, Loss=1.4436456, Gaussian number=4572793, print grad=0.0009234806639142334, Depth Loss=0.0000000]
Training progress:  91%|█████████ | 1820/2000 [06:38<00:39,  4.60it/s, Loss=1.4436456, Gaussian number=4572793, print grad=0.0009234806639142334, Depth Loss=0.0000000]
Training progress:  91%|█████████ | 1820/2000 [06:41<00:39,  4.60it/s, Loss=1.3848979, Gaussian number=4572793, print grad=0.0012806191807612777, Depth Loss=0.0000000]
Training progress:  92%|█████████▏| 1830/2000 [06:41<00:36,  4.60it/s, Loss=1.3848979, Gaussian number=4572793, print grad=0.0012806191807612777, Depth Loss=0.0000000]
Training progress:  92%|█████████▏| 1830/2000 [06:43<00:36,  4.60it/s, Loss=1.2981273, Gaussian number=4572793, print grad=0.0017914284253492951, Depth Loss=0.0000000]
Training progress:  92%|█████████▏| 1840/2000 [06:43<00:34,  4.59it/s, Loss=1.2981273, Gaussian number=4572793, print grad=0.0017914284253492951, Depth Loss=0.0000000]
Training progress:  92%|█████████▏| 1840/2000 [06:45<00:34,  4.59it/s, Loss=1.4202918, Gaussian number=4572793, print grad=0.0022582807578146458, Depth Loss=0.0000000]
Training progress:  92%|█████████▎| 1850/2000 [06:45<00:32,  4.57it/s, Loss=1.4202918, Gaussian number=4572793, print grad=0.0022582807578146458, Depth Loss=0.0000000]
Training progress:  92%|█████████▎| 1850/2000 [06:47<00:32,  4.57it/s, Loss=1.6085617, Gaussian number=4572793, print grad=0.0027178586460649967, Depth Loss=0.0000000]
Training progress:  93%|█████████▎| 1860/2000 [06:47<00:30,  4.59it/s, Loss=1.6085617, Gaussian number=4572793, print grad=0.0027178586460649967, Depth Loss=0.0000000]
Training progress:  93%|█████████▎| 1860/2000 [06:49<00:30,  4.59it/s, Loss=1.8168337, Gaussian number=4572793, print grad=0.003227576846256852, Depth Loss=0.0000000] 
Training progress:  94%|█████████▎| 1870/2000 [06:49<00:28,  4.55it/s, Loss=1.8168337, Gaussian number=4572793, print grad=0.003227576846256852, Depth Loss=0.0000000]
Training progress:  94%|█████████▎| 1870/2000 [06:52<00:28,  4.55it/s, Loss=1.2094496, Gaussian number=4572793, print grad=0.003759185317903757, Depth Loss=0.0000000]
Training progress:  94%|█████████▍| 1880/2000 [06:52<00:27,  4.41it/s, Loss=1.2094496, Gaussian number=4572793, print grad=0.003759185317903757, Depth Loss=0.0000000]
Training progress:  94%|█████████▍| 1880/2000 [06:54<00:27,  4.41it/s, Loss=1.6203473, Gaussian number=4572793, print grad=0.00427349703386426, Depth Loss=0.0000000] 
Training progress:  94%|█████████▍| 1890/2000 [06:54<00:24,  4.45it/s, Loss=1.6203473, Gaussian number=4572793, print grad=0.00427349703386426, Depth Loss=0.0000000]
Training progress:  94%|█████████▍| 1890/2000 [06:56<00:24,  4.45it/s, Loss=1.7497245, Gaussian number=4572793, print grad=0.004700568504631519, Depth Loss=0.0000000]
Training progress:  95%|█████████▌| 1900/2000 [06:56<00:22,  4.49it/s, Loss=1.7497245, Gaussian number=4572793, print grad=0.004700568504631519, Depth Loss=0.0000000]
Training progress:  95%|█████████▌| 1900/2000 [06:58<00:22,  4.49it/s, Loss=1.5655930, Gaussian number=5026491, print grad=0.0003667010460048914, Depth Loss=0.0000000]
Training progress:  96%|█████████▌| 1910/2000 [06:58<00:20,  4.47it/s, Loss=1.5655930, Gaussian number=5026491, print grad=0.0003667010460048914, Depth Loss=0.0000000]
Training progress:  96%|█████████▌| 1910/2000 [07:01<00:20,  4.47it/s, Loss=1.5474936, Gaussian number=5026491, print grad=0.0008198645664379001, Depth Loss=0.0000000]
Training progress:  96%|█████████▌| 1920/2000 [07:01<00:19,  4.12it/s, Loss=1.5474936, Gaussian number=5026491, print grad=0.0008198645664379001, Depth Loss=0.0000000]
Training progress:  96%|█████████▌| 1920/2000 [07:04<00:19,  4.12it/s, Loss=1.1827413, Gaussian number=5026491, print grad=0.0012793366331607103, Depth Loss=0.0000000]
Training progress:  96%|█████████▋| 1930/2000 [07:04<00:16,  4.20it/s, Loss=1.1827413, Gaussian number=5026491, print grad=0.0012793366331607103, Depth Loss=0.0000000]
Training progress:  96%|█████████▋| 1930/2000 [07:06<00:16,  4.20it/s, Loss=1.6320667, Gaussian number=5026491, print grad=0.001742594875395298, Depth Loss=0.0000000] 
Training progress:  97%|█████████▋| 1940/2000 [07:06<00:14,  4.28it/s, Loss=1.6320667, Gaussian number=5026491, print grad=0.001742594875395298, Depth Loss=0.0000000]
Training progress:  97%|█████████▋| 1940/2000 [07:08<00:14,  4.28it/s, Loss=1.3294154, Gaussian number=5026491, print grad=0.0021968737710267305, Depth Loss=0.0000000]
Training progress:  98%|█████████▊| 1950/2000 [07:08<00:11,  4.30it/s, Loss=1.3294154, Gaussian number=5026491, print grad=0.0021968737710267305, Depth Loss=0.0000000]
Training progress:  98%|█████████▊| 1950/2000 [07:11<00:11,  4.30it/s, Loss=1.8861596, Gaussian number=5026491, print grad=0.002606266411021352, Depth Loss=0.0000000] 
Training progress:  98%|█████████▊| 1960/2000 [07:11<00:09,  4.01it/s, Loss=1.8861596, Gaussian number=5026491, print grad=0.002606266411021352, Depth Loss=0.0000000]
Training progress:  98%|█████████▊| 1960/2000 [07:13<00:09,  4.01it/s, Loss=1.6745160, Gaussian number=5026491, print grad=0.003021456301212311, Depth Loss=0.0000000]
Training progress:  98%|█████████▊| 1970/2000 [07:13<00:07,  4.03it/s, Loss=1.6745160, Gaussian number=5026491, print grad=0.003021456301212311, Depth Loss=0.0000000]
Training progress:  98%|█████████▊| 1970/2000 [07:16<00:07,  4.03it/s, Loss=1.5607828, Gaussian number=5026491, print grad=0.0035060103982686996, Depth Loss=0.0000000]
Training progress:  99%|█████████▉| 1980/2000 [07:16<00:04,  4.05it/s, Loss=1.5607828, Gaussian number=5026491, print grad=0.0035060103982686996, Depth Loss=0.0000000]
Training progress:  99%|█████████▉| 1980/2000 [07:18<00:04,  4.05it/s, Loss=1.4731506, Gaussian number=5026491, print grad=0.003907299600541592, Depth Loss=0.0000000] 
Training progress: 100%|█████████▉| 1990/2000 [07:18<00:02,  4.06it/s, Loss=1.4731506, Gaussian number=5026491, print grad=0.003907299600541592, Depth Loss=0.0000000]
Training progress: 100%|█████████▉| 1990/2000 [07:21<00:02,  4.06it/s, Loss=1.0921935, Gaussian number=5026491, print grad=0.004430052358657122, Depth Loss=0.0000000]
Training progress: 100%|██████████| 2000/2000 [07:21<00:00,  4.06it/s, Loss=1.0921935, Gaussian number=5026491, print grad=0.004430052358657122, Depth Loss=0.0000000]
Training progress: 100%|██████████| 2000/2000 [07:21<00:00,  4.53it/s, Loss=1.0921935, Gaussian number=5026491, print grad=0.004430052358657122, Depth Loss=0.0000000]
Iteration 100 [14/11 19:16:24]

[ITER 100] Evaluating test: WD 1.978819, PSNR 12.9380 [14/11 19:16:26]

[ITER 100] Evaluating train: WD 2.061503, PSNR 13.2935 [14/11 19:16:27]
Gaussian number:182686,print gradients:0.00028101218049414456 [14/11 19:16:27]
Iteration 200 [14/11 19:16:48]

[ITER 200] Evaluating test: WD 1.751075, PSNR 14.2266 [14/11 19:16:51]

[ITER 200] Evaluating train: WD 1.778257, PSNR 14.3852 [14/11 19:16:51]
Gaussian number:182686,print gradients:0.00036239359178580344 [14/11 19:16:51]
Iteration 300 [14/11 19:17:12]

[ITER 300] Evaluating test: WD 1.598902, PSNR 14.9449 [14/11 19:17:15]

[ITER 300] Evaluating train: WD 1.617731, PSNR 15.2082 [14/11 19:17:15]
Gaussian number:182686,print gradients:0.00041331228567287326 [14/11 19:17:15]
Iteration 400 [14/11 19:17:36]

[ITER 400] Evaluating test: WD 1.533848, PSNR 15.3911 [14/11 19:17:39]

[ITER 400] Evaluating train: WD 1.552829, PSNR 15.7812 [14/11 19:17:39]
Gaussian number:182686,print gradients:0.0004463086661417037 [14/11 19:17:39]
Iteration 500 [14/11 19:18:01]

[ITER 500] Evaluating test: WD 1.443345, PSNR 15.8271 [14/11 19:18:04]

[ITER 500] Evaluating train: WD 1.551986, PSNR 15.8614 [14/11 19:18:04]
Gaussian number:182686,print gradients:0.00048240835894830525 [14/11 19:18:04]
Iteration 600 [14/11 19:18:25]

[ITER 600] Evaluating test: WD 1.381083, PSNR 16.0761 [14/11 19:18:27]

[ITER 600] Evaluating train: WD 1.444409, PSNR 16.1087 [14/11 19:18:27]
Gaussian number:182686,print gradients:0.0005078837275505066 [14/11 19:18:27]
Iteration 700 [14/11 19:18:45]

[ITER 700] Evaluating test: WD 1.398513, PSNR 16.1786 [14/11 19:18:47]

[ITER 700] Evaluating train: WD 1.448025, PSNR 16.3856 [14/11 19:18:48]
Gaussian number:306434,print gradients:0.0005113154766149819 [14/11 19:18:48]
Iteration 800 [14/11 19:19:05]

[ITER 800] Evaluating test: WD 1.337043, PSNR 16.3140 [14/11 19:19:08]

[ITER 800] Evaluating train: WD 1.402804, PSNR 16.5809 [14/11 19:19:08]
Gaussian number:481289,print gradients:0.0003667015116661787 [14/11 19:19:08]
Iteration 900 [14/11 19:19:26]

[ITER 900] Evaluating test: WD 1.340832, PSNR 16.5207 [14/11 19:19:28]

[ITER 900] Evaluating train: WD 1.465124, PSNR 16.5516 [14/11 19:19:28]
Gaussian number:687089,print gradients:nan [14/11 19:19:28]
Iteration 1000 [14/11 19:19:47]

[ITER 1000] Evaluating test: WD 1.332551, PSNR 16.5309 [14/11 19:19:49]

[ITER 1000] Evaluating train: WD 1.447836, PSNR 16.4070 [14/11 19:19:49]
Gaussian number:936500,print gradients:nan [14/11 19:19:49]
Iteration 1100 [14/11 19:20:08]
Iteration 1200 [14/11 19:20:27]
Iteration 1300 [14/11 19:20:46]
Iteration 1400 [14/11 19:21:06]
Iteration 1500 [14/11 19:21:26]

[ITER 1500] Evaluating test: WD 1.385789, PSNR 16.0269 [14/11 19:21:29]

[ITER 1500] Evaluating train: WD 1.545403, PSNR 16.4869 [14/11 19:21:29]
Gaussian number:2738045,print gradients:nan [14/11 19:21:29]
Iteration 1600 [14/11 19:21:49]
Iteration 1700 [14/11 19:22:11]
Iteration 1800 [14/11 19:22:32]
Iteration 1900 [14/11 19:22:54]
Iteration 2000 [14/11 19:23:19]

[ITER 2000] Evaluating test: WD 1.525045, PSNR 15.5745 [14/11 19:23:22]

[ITER 2000] Evaluating train: WD 1.838647, PSNR 14.9429 [14/11 19:23:22]
Gaussian number:5026491,print gradients:nan [14/11 19:23:22]

[ITER 2000] Saving Gaussians [14/11 19:23:22]

Training complete. [14/11 19:24:20]
